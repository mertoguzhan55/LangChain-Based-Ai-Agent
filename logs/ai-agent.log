[2025-04-13 13:34:03] | DEBUG | [app:main:10] | [############ AI AGENT CONFIGURATIONS ############]
[2025-04-13 13:34:03] | DEBUG | [app:main:11] | [{'project': {'name': 'AI AGENT', 'description': 'Ai Agent Repository', 'company': 'Oguzhan', 'author': 'Mert Oğuzhan'}, 'logger': {'filepath': './logs/ai-agent.log', 'rotation': '50MB'}, 'agent': {'language_model_name': 'llama-3.3-70b-versatile', 'model_provider': 'groq'}}]
[2025-04-13 13:34:20] | DEBUG | [app:main:10] | [############ AI AGENT CONFIGURATIONS ############]
[2025-04-13 13:34:20] | DEBUG | [app:main:11] | [{'project': {'name': 'AI AGENT', 'description': 'Ai Agent Repository', 'company': 'Oguzhan', 'author': 'Mert Oğuzhan'}, 'logger': {'filepath': './logs/ai-agent.log', 'rotation': '50MB'}, 'agent': {'language_model_name': 'llama-3.3-70b-versatile', 'model_provider': 'groq'}}]
[2025-04-13 13:38:55] | DEBUG | [app:main:10] | [############ AI AGENT CONFIGURATIONS ############]
[2025-04-13 13:38:55] | DEBUG | [app:main:11] | [{'project': {'name': 'AI AGENT', 'description': 'Ai Agent Repository', 'company': 'Oguzhan', 'author': 'Mert Oğuzhan'}, 'logger': {'filepath': './logs/ai-agent.log', 'rotation': '50MB'}, 'agent': {'language_model_name': 'llama-3.3-70b-versatile', 'model_provider': 'groq'}}]
[2025-04-13 13:38:55] | INFO | [agent:__post_init__:22] | [post init worked successsfully!]
[2025-04-13 15:19:01] | DEBUG | [app:main:10] | [############ AI AGENT CONFIGURATIONS ############]
[2025-04-13 15:19:01] | DEBUG | [app:main:11] | [{'project': {'name': 'AI AGENT', 'description': 'Ai Agent Repository', 'company': 'Oguzhan', 'author': 'Mert Oğuzhan'}, 'logger': {'filepath': './logs/ai-agent.log', 'rotation': '50MB'}, 'agent': {'language_model_name': 'llama-3.3-70b-versatile', 'model_provider': 'groq'}}]
[2025-04-13 15:19:25] | DEBUG | [app:main:10] | [############ AI AGENT CONFIGURATIONS ############]
[2025-04-13 15:19:25] | DEBUG | [app:main:11] | [{'project': {'name': 'AI AGENT', 'description': 'Ai Agent Repository', 'company': 'Oguzhan', 'author': 'Mert Oğuzhan'}, 'logger': {'filepath': './logs/ai-agent.log', 'rotation': '50MB'}, 'agent': {'language_model_name': 'llama-3.3-70b-versatile', 'model_provider': 'groq'}}]
[2025-04-13 15:19:25] | INFO | [agent:__post_init__:20] | [post init worked successsfully!]
[2025-04-13 15:19:35] | INFO | [agent:run_agent:42] | [The output was generated successfully.]
[2025-04-13 15:19:44] | DEBUG | [app:main:10] | [############ AI AGENT CONFIGURATIONS ############]
[2025-04-13 15:19:44] | DEBUG | [app:main:11] | [{'project': {'name': 'AI AGENT', 'description': 'Ai Agent Repository', 'company': 'Oguzhan', 'author': 'Mert Oğuzhan'}, 'logger': {'filepath': './logs/ai-agent.log', 'rotation': '50MB'}, 'agent': {'language_model_name': 'llama-3.3-70b-versatile', 'model_provider': 'groq'}}]
[2025-04-13 15:19:44] | INFO | [agent:__post_init__:20] | [post init worked successsfully!]
[2025-04-13 15:19:55] | INFO | [agent:run_agent:42] | [The output was generated successfully.]
[2025-04-13 15:57:31] | DEBUG | [app:main:10] | [############ AI AGENT CONFIGURATIONS ############]
[2025-04-13 15:57:31] | DEBUG | [app:main:11] | [{'project': {'name': 'AI AGENT', 'description': 'Ai Agent Repository', 'company': 'Oguzhan', 'author': 'Mert Oğuzhan'}, 'logger': {'filepath': './logs/ai-agent.log', 'rotation': '50MB'}, 'agent': {'language_model_name': 'llama-3.3-70b-versatile', 'model_provider': 'groq'}}]
[2025-04-13 15:57:32] | INFO | [agent:__post_init__:20] | [post init worked successsfully!]
[2025-04-13 15:57:39] | INFO | [agent:run_agent:43] | [The output was generated successfully.]
[2025-04-13 15:58:30] | DEBUG | [app:main:10] | [############ AI AGENT CONFIGURATIONS ############]
[2025-04-13 15:58:30] | DEBUG | [app:main:11] | [{'project': {'name': 'AI AGENT', 'description': 'Ai Agent Repository', 'company': 'Oguzhan', 'author': 'Mert Oğuzhan'}, 'logger': {'filepath': './logs/ai-agent.log', 'rotation': '50MB'}, 'agent': {'language_model_name': 'llama-3.3-70b-versatile', 'model_provider': 'groq'}}]
[2025-04-13 15:58:30] | INFO | [agent:__post_init__:20] | [post init worked successsfully!]
[2025-04-13 15:58:42] | INFO | [agent:run_agent:43] | [The output was generated successfully.]
[2025-04-13 16:19:40] | DEBUG | [app:main:10] | [############ AI AGENT CONFIGURATIONS ############]
[2025-04-13 16:19:40] | DEBUG | [app:main:11] | [{'project': {'name': 'AI AGENT', 'description': 'Ai Agent Repository', 'company': 'Oguzhan', 'author': 'Mert Oğuzhan'}, 'logger': {'filepath': './logs/ai-agent.log', 'rotation': '50MB'}, 'agent': {'language_model_name': 'llama-3.3-70b-versatile', 'model_provider': 'groq'}}]
[2025-04-13 16:19:40] | INFO | [agent:__post_init__:20] | [post init worked successsfully!]
[2025-04-13 16:19:49] | INFO | [agent:run_agent:43] | [The output was generated successfully.]
[2025-04-13 16:59:03] | DEBUG | [app:main:10] | [############ AI AGENT CONFIGURATIONS ############]
[2025-04-13 16:59:03] | DEBUG | [app:main:11] | [{'project': {'name': 'AI AGENT', 'description': 'Ai Agent Repository', 'company': 'Oguzhan', 'author': 'Mert Oğuzhan'}, 'logger': {'filepath': './logs/ai-agent.log', 'rotation': '50MB'}, 'agent': {'language_model_name': 'llama-3.3-70b-versatile', 'model_provider': 'groq'}, 'fastapi': {'host': '0.0.0.0', 'port': 9090, 'reload': True}}]
[2025-04-13 16:59:24] | DEBUG | [app:main:10] | [############ AI AGENT CONFIGURATIONS ############]
[2025-04-13 16:59:24] | DEBUG | [app:main:11] | [{'project': {'name': 'AI AGENT', 'description': 'Ai Agent Repository', 'company': 'Oguzhan', 'author': 'Mert Oğuzhan'}, 'logger': {'filepath': './logs/ai-agent.log', 'rotation': '50MB'}, 'agent': {'language_model_name': 'llama-3.3-70b-versatile', 'model_provider': 'groq'}, 'fastapi': {'host': '0.0.0.0', 'port': 9090, 'reload': True}}]
[2025-04-13 16:59:24] | INFO | [fastapi:run:25] | [Server Initialized!]
[2025-04-13 17:05:36] | DEBUG | [app:main:10] | [############ AI AGENT CONFIGURATIONS ############]
[2025-04-13 17:05:36] | DEBUG | [app:main:11] | [{'project': {'name': 'AI AGENT', 'description': 'Ai Agent Repository', 'company': 'Oguzhan', 'author': 'Mert Oğuzhan'}, 'logger': {'filepath': './logs/ai-agent.log', 'rotation': '50MB'}, 'agent': {'language_model_name': 'llama-3.3-70b-versatile', 'model_provider': 'groq'}, 'fastapi': {'host': '0.0.0.0', 'port': 9090, 'reload': True}}]
[2025-04-13 17:05:36] | INFO | [agent:__post_init__:20] | [post init worked successsfully!]
[2025-04-13 17:05:36] | INFO | [fastapi:run:25] | [Server Initialized!]
[2025-04-13 20:27:51] | DEBUG | [app:main:10] | [############ AI AGENT CONFIGURATIONS ############]
[2025-04-13 20:27:51] | DEBUG | [app:main:11] | [{'project': {'name': 'AI AGENT', 'description': 'Ai Agent Repository', 'company': 'Oguzhan', 'author': 'Mert Oğuzhan'}, 'logger': {'filepath': './logs/ai-agent.log', 'rotation': '50MB'}, 'agent': {'language_model_name': 'llama-3.3-70b-versatile', 'model_provider': 'groq'}, 'fastapi': {'host': '0.0.0.0', 'port': 9090, 'reload': True}}]
[2025-04-13 20:27:51] | INFO | [agent:__post_init__:20] | [post init worked successsfully!]
[2025-04-13 20:27:51] | INFO | [fastapi:run:25] | [Server Initialized!]
[2025-04-13 21:38:08] | DEBUG | [app:main:10] | [############ AI AGENT CONFIGURATIONS ############]
[2025-04-13 21:38:08] | DEBUG | [app:main:11] | [{'project': {'name': 'AI AGENT', 'description': 'Ai Agent Repository', 'company': 'Oguzhan', 'author': 'Mert Oğuzhan'}, 'logger': {'filepath': './logs/ai-agent.log', 'rotation': '50MB'}, 'agent': {'language_model_name': 'llama-3.3-70b-versatile', 'model_provider': 'groq'}, 'fastapi': {'host': '0.0.0.0', 'port': 9090, 'reload': True}}]
[2025-04-13 21:38:08] | INFO | [agent:__post_init__:20] | [post init worked successsfully!]
[2025-04-13 21:38:08] | INFO | [fastapi:run:24] | [Server Initialized!]
[2025-04-13 21:39:29] | DEBUG | [app:main:10] | [############ AI AGENT CONFIGURATIONS ############]
[2025-04-13 21:39:29] | DEBUG | [app:main:11] | [{'project': {'name': 'AI AGENT', 'description': 'Ai Agent Repository', 'company': 'Oguzhan', 'author': 'Mert Oğuzhan'}, 'logger': {'filepath': './logs/ai-agent.log', 'rotation': '50MB'}, 'agent': {'language_model_name': 'llama-3.3-70b-versatile', 'model_provider': 'groq'}, 'fastapi': {'host': '0.0.0.0', 'port': 9090, 'reload': True}}]
[2025-04-13 21:39:29] | INFO | [agent:__post_init__:20] | [post init worked successsfully!]
[2025-04-13 21:39:29] | INFO | [fastapi:run:24] | [Server Initialized!]
[2025-04-13 21:43:46] | DEBUG | [app:main:10] | [############ AI AGENT CONFIGURATIONS ############]
[2025-04-13 21:43:46] | DEBUG | [app:main:11] | [{'project': {'name': 'AI AGENT', 'description': 'Ai Agent Repository', 'company': 'Oguzhan', 'author': 'Mert Oğuzhan'}, 'logger': {'filepath': './logs/ai-agent.log', 'rotation': '50MB'}, 'agent': {'language_model_name': 'llama-3.3-70b-versatile', 'model_provider': 'groq'}, 'fastapi': {'host': '0.0.0.0', 'port': 9090, 'reload': True}}]
[2025-04-13 21:43:47] | INFO | [agent:__post_init__:20] | [post init worked successsfully!]
[2025-04-13 21:43:47] | INFO | [fastapi:run:24] | [Server Initialized!]
[2025-04-13 21:44:41] | DEBUG | [app:main:10] | [############ AI AGENT CONFIGURATIONS ############]
[2025-04-13 21:44:41] | DEBUG | [app:main:11] | [{'project': {'name': 'AI AGENT', 'description': 'Ai Agent Repository', 'company': 'Oguzhan', 'author': 'Mert Oğuzhan'}, 'logger': {'filepath': './logs/ai-agent.log', 'rotation': '50MB'}, 'agent': {'language_model_name': 'llama-3.3-70b-versatile', 'model_provider': 'groq'}, 'fastapi': {'host': '0.0.0.0', 'port': 9090, 'reload': True}}]
[2025-04-13 21:44:42] | INFO | [agent:__post_init__:20] | [post init worked successsfully!]
[2025-04-13 21:44:42] | INFO | [fastapi:run:24] | [Server Initialized!]
[2025-04-13 21:48:24] | DEBUG | [app:main:10] | [############ AI AGENT CONFIGURATIONS ############]
[2025-04-13 21:48:24] | DEBUG | [app:main:11] | [{'project': {'name': 'AI AGENT', 'description': 'Ai Agent Repository', 'company': 'Oguzhan', 'author': 'Mert Oğuzhan'}, 'logger': {'filepath': './logs/ai-agent.log', 'rotation': '50MB'}, 'agent': {'language_model_name': 'llama-3.3-70b-versatile', 'model_provider': 'groq'}, 'fastapi': {'host': '0.0.0.0', 'port': 9090, 'reload': True}}]
[2025-04-13 21:48:24] | INFO | [agent:__post_init__:20] | [post init worked successsfully!]
[2025-04-13 21:48:24] | INFO | [fastapi:run:24] | [Server Initialized!]
[2025-04-13 21:49:00] | DEBUG | [app:main:10] | [############ AI AGENT CONFIGURATIONS ############]
[2025-04-13 21:49:00] | DEBUG | [app:main:11] | [{'project': {'name': 'AI AGENT', 'description': 'Ai Agent Repository', 'company': 'Oguzhan', 'author': 'Mert Oğuzhan'}, 'logger': {'filepath': './logs/ai-agent.log', 'rotation': '50MB'}, 'agent': {'language_model_name': 'llama-3.3-70b-versatile', 'model_provider': 'groq'}, 'fastapi': {'host': '0.0.0.0', 'port': 9090, 'reload': True}}]
[2025-04-13 21:49:00] | INFO | [agent:__post_init__:20] | [post init worked successsfully!]
[2025-04-13 21:49:00] | INFO | [fastapi:run:24] | [Server Initialized!]
[2025-04-13 21:54:50] | DEBUG | [app:main:10] | [############ AI AGENT CONFIGURATIONS ############]
[2025-04-13 21:54:50] | DEBUG | [app:main:11] | [{'project': {'name': 'AI AGENT', 'description': 'Ai Agent Repository', 'company': 'Oguzhan', 'author': 'Mert Oğuzhan'}, 'logger': {'filepath': './logs/ai-agent.log', 'rotation': '50MB'}, 'agent': {'language_model_name': 'llama-3.3-70b-versatile', 'model_provider': 'groq'}, 'fastapi': {'host': '0.0.0.0', 'port': 9090, 'reload': True}}]
[2025-04-13 21:54:50] | INFO | [agent:__post_init__:20] | [post init worked successsfully!]
[2025-04-13 21:54:51] | INFO | [fastapi:run:24] | [Server Initialized!]
[2025-04-13 21:55:50] | DEBUG | [app:main:10] | [############ AI AGENT CONFIGURATIONS ############]
[2025-04-13 21:55:50] | DEBUG | [app:main:11] | [{'project': {'name': 'AI AGENT', 'description': 'Ai Agent Repository', 'company': 'Oguzhan', 'author': 'Mert Oğuzhan'}, 'logger': {'filepath': './logs/ai-agent.log', 'rotation': '50MB'}, 'agent': {'language_model_name': 'llama-3.3-70b-versatile', 'model_provider': 'groq'}, 'fastapi': {'host': '0.0.0.0', 'port': 9090, 'reload': True}}]
[2025-04-13 21:55:50] | INFO | [agent:__post_init__:20] | [post init worked successsfully!]
[2025-04-13 21:55:51] | INFO | [fastapi:run:24] | [Server Initialized!]
[2025-04-13 21:56:31] | DEBUG | [app:main:10] | [############ AI AGENT CONFIGURATIONS ############]
[2025-04-13 21:56:31] | DEBUG | [app:main:11] | [{'project': {'name': 'AI AGENT', 'description': 'Ai Agent Repository', 'company': 'Oguzhan', 'author': 'Mert Oğuzhan'}, 'logger': {'filepath': './logs/ai-agent.log', 'rotation': '50MB'}, 'agent': {'language_model_name': 'llama-3.3-70b-versatile', 'model_provider': 'groq'}, 'fastapi': {'host': '0.0.0.0', 'port': 9090, 'reload': True}}]
[2025-04-13 21:56:31] | INFO | [agent:__post_init__:20] | [post init worked successsfully!]
[2025-04-13 21:56:32] | INFO | [fastapi:run:24] | [Server Initialized!]
[2025-04-14 22:43:26] | DEBUG | [app:main:12] | [############ AI AGENT CONFIGURATIONS ############]
[2025-04-14 22:43:26] | DEBUG | [app:main:13] | [{'project': {'name': 'AI AGENT', 'description': 'Ai Agent Repository', 'company': 'Oguzhan', 'author': 'Mert Oğuzhan'}, 'logger': {'filepath': './logs/ai-agent.log', 'rotation': '50MB'}, 'agent': {'language_model_name': 'llama-3.3-70b-versatile', 'model_provider': 'groq'}, 'fastapi': {'host': '0.0.0.0', 'port': 9090, 'reload': True}}]
[2025-04-14 22:43:41] | DEBUG | [app:main:12] | [############ AI AGENT CONFIGURATIONS ############]
[2025-04-14 22:43:41] | DEBUG | [app:main:13] | [{'project': {'name': 'AI AGENT', 'description': 'Ai Agent Repository', 'company': 'Oguzhan', 'author': 'Mert Oğuzhan'}, 'logger': {'filepath': './logs/ai-agent.log', 'rotation': '50MB'}, 'agent': {'language_model_name': 'llama-3.3-70b-versatile', 'model_provider': 'groq'}, 'fastapi': {'host': '0.0.0.0', 'port': 9090, 'reload': True}}]
[2025-04-14 22:43:41] | INFO | [agent:__post_init__:20] | [post init worked successsfully!]
[2025-04-14 22:43:50] | DEBUG | [app:main:12] | [############ AI AGENT CONFIGURATIONS ############]
[2025-04-14 22:43:50] | DEBUG | [app:main:13] | [{'project': {'name': 'AI AGENT', 'description': 'Ai Agent Repository', 'company': 'Oguzhan', 'author': 'Mert Oğuzhan'}, 'logger': {'filepath': './logs/ai-agent.log', 'rotation': '50MB'}, 'agent': {'language_model_name': 'llama-3.3-70b-versatile', 'model_provider': 'groq'}, 'fastapi': {'host': '0.0.0.0', 'port': 9090, 'reload': True}}]
[2025-04-14 22:43:50] | INFO | [agent:__post_init__:20] | [post init worked successsfully!]
[2025-04-14 22:43:55] | DEBUG | [app:main:12] | [############ AI AGENT CONFIGURATIONS ############]
[2025-04-14 22:43:55] | DEBUG | [app:main:13] | [{'project': {'name': 'AI AGENT', 'description': 'Ai Agent Repository', 'company': 'Oguzhan', 'author': 'Mert Oğuzhan'}, 'logger': {'filepath': './logs/ai-agent.log', 'rotation': '50MB'}, 'agent': {'language_model_name': 'llama-3.3-70b-versatile', 'model_provider': 'groq'}, 'fastapi': {'host': '0.0.0.0', 'port': 9090, 'reload': True}}]
[2025-04-14 22:43:55] | INFO | [agent:__post_init__:20] | [post init worked successsfully!]
[2025-04-14 22:43:55] | INFO | [fastapi:run:40] | [Server Initialized!]
[2025-04-14 22:45:12] | DEBUG | [app:main:12] | [############ AI AGENT CONFIGURATIONS ############]
[2025-04-14 22:45:12] | DEBUG | [app:main:13] | [{'project': {'name': 'AI AGENT', 'description': 'Ai Agent Repository', 'company': 'Oguzhan', 'author': 'Mert Oğuzhan'}, 'logger': {'filepath': './logs/ai-agent.log', 'rotation': '50MB'}, 'agent': {'language_model_name': 'llama-3.3-70b-versatile', 'model_provider': 'groq'}, 'fastapi': {'host': '0.0.0.0', 'port': 9090, 'reload': True}}]
[2025-04-14 22:45:12] | INFO | [agent:__post_init__:20] | [post init worked successsfully!]
[2025-04-14 22:45:12] | INFO | [fastapi:run:40] | [Server Initialized!]
[2025-04-14 22:45:55] | DEBUG | [app:main:12] | [############ AI AGENT CONFIGURATIONS ############]
[2025-04-14 22:45:55] | DEBUG | [app:main:13] | [{'project': {'name': 'AI AGENT', 'description': 'Ai Agent Repository', 'company': 'Oguzhan', 'author': 'Mert Oğuzhan'}, 'logger': {'filepath': './logs/ai-agent.log', 'rotation': '50MB'}, 'agent': {'language_model_name': 'llama-3.3-70b-versatile', 'model_provider': 'groq'}, 'fastapi': {'host': '0.0.0.0', 'port': 9090, 'reload': True}}]
[2025-04-14 22:45:56] | INFO | [agent:__post_init__:20] | [post init worked successsfully!]
[2025-04-14 22:45:56] | INFO | [fastapi:run:40] | [Server Initialized!]
[2025-04-14 22:49:08] | DEBUG | [app:main:12] | [############ AI AGENT CONFIGURATIONS ############]
[2025-04-14 22:49:08] | DEBUG | [app:main:13] | [{'project': {'name': 'AI AGENT', 'description': 'Ai Agent Repository', 'company': 'Oguzhan', 'author': 'Mert Oğuzhan'}, 'logger': {'filepath': './logs/ai-agent.log', 'rotation': '50MB'}, 'agent': {'language_model_name': 'llama-3.3-70b-versatile', 'model_provider': 'groq'}, 'fastapi': {'host': '0.0.0.0', 'port': 9090, 'reload': True}}]
[2025-04-14 22:49:08] | INFO | [agent:__post_init__:20] | [post init worked successsfully!]
[2025-04-14 22:49:08] | INFO | [fastapi:run:41] | [Server Initialized!]
[2025-04-14 22:50:44] | DEBUG | [app:main:12] | [############ AI AGENT CONFIGURATIONS ############]
[2025-04-14 22:50:44] | DEBUG | [app:main:13] | [{'project': {'name': 'AI AGENT', 'description': 'Ai Agent Repository', 'company': 'Oguzhan', 'author': 'Mert Oğuzhan'}, 'logger': {'filepath': './logs/ai-agent.log', 'rotation': '50MB'}, 'agent': {'language_model_name': 'llama-3.3-70b-versatile', 'model_provider': 'groq'}, 'fastapi': {'host': '0.0.0.0', 'port': 9090, 'reload': True}}]
[2025-04-14 22:50:44] | INFO | [agent:__post_init__:20] | [post init worked successsfully!]
[2025-04-14 22:50:44] | INFO | [fastapi:run:41] | [Server Initialized!]
[2025-04-14 22:51:00] | DEBUG | [app:main:12] | [############ AI AGENT CONFIGURATIONS ############]
[2025-04-14 22:51:00] | DEBUG | [app:main:13] | [{'project': {'name': 'AI AGENT', 'description': 'Ai Agent Repository', 'company': 'Oguzhan', 'author': 'Mert Oğuzhan'}, 'logger': {'filepath': './logs/ai-agent.log', 'rotation': '50MB'}, 'agent': {'language_model_name': 'llama-3.3-70b-versatile', 'model_provider': 'groq'}, 'fastapi': {'host': '0.0.0.0', 'port': 9090, 'reload': True}}]
[2025-04-14 22:51:01] | INFO | [agent:__post_init__:20] | [post init worked successsfully!]
[2025-04-14 22:51:01] | INFO | [fastapi:run:41] | [Server Initialized!]
[2025-04-14 22:53:36] | DEBUG | [app:main:12] | [############ AI AGENT CONFIGURATIONS ############]
[2025-04-14 22:53:36] | DEBUG | [app:main:13] | [{'project': {'name': 'AI AGENT', 'description': 'Ai Agent Repository', 'company': 'Oguzhan', 'author': 'Mert Oğuzhan'}, 'logger': {'filepath': './logs/ai-agent.log', 'rotation': '50MB'}, 'agent': {'language_model_name': 'llama-3.3-70b-versatile', 'model_provider': 'groq'}, 'fastapi': {'host': '0.0.0.0', 'port': 9090, 'reload': True}}]
[2025-04-14 22:53:36] | INFO | [agent:__post_init__:20] | [post init worked successsfully!]
[2025-04-14 22:53:36] | INFO | [fastapi:run:41] | [Server Initialized!]
[2025-04-14 22:54:22] | DEBUG | [app:main:12] | [############ AI AGENT CONFIGURATIONS ############]
[2025-04-14 22:54:22] | DEBUG | [app:main:13] | [{'project': {'name': 'AI AGENT', 'description': 'Ai Agent Repository', 'company': 'Oguzhan', 'author': 'Mert Oğuzhan'}, 'logger': {'filepath': './logs/ai-agent.log', 'rotation': '50MB'}, 'agent': {'language_model_name': 'llama-3.3-70b-versatile', 'model_provider': 'groq'}, 'fastapi': {'host': '0.0.0.0', 'port': 9090, 'reload': True}}]
[2025-04-14 22:54:22] | INFO | [agent:__post_init__:20] | [post init worked successsfully!]
[2025-04-14 22:54:22] | INFO | [fastapi:run:42] | [Server Initialized!]
[2025-04-14 22:58:21] | DEBUG | [app:main:12] | [############ AI AGENT CONFIGURATIONS ############]
[2025-04-14 22:58:21] | DEBUG | [app:main:13] | [{'project': {'name': 'AI AGENT', 'description': 'Ai Agent Repository', 'company': 'Oguzhan', 'author': 'Mert Oğuzhan'}, 'logger': {'filepath': './logs/ai-agent.log', 'rotation': '50MB'}, 'agent': {'language_model_name': 'llama-3.3-70b-versatile', 'model_provider': 'groq'}, 'fastapi': {'host': '0.0.0.0', 'port': 9090, 'reload': True}}]
[2025-04-14 22:58:21] | INFO | [agent:__post_init__:20] | [post init worked successsfully!]
[2025-04-14 22:58:21] | INFO | [fastapi:run:42] | [Server Initialized!]
[2025-04-14 22:59:25] | DEBUG | [app:main:12] | [############ AI AGENT CONFIGURATIONS ############]
[2025-04-14 22:59:25] | DEBUG | [app:main:13] | [{'project': {'name': 'AI AGENT', 'description': 'Ai Agent Repository', 'company': 'Oguzhan', 'author': 'Mert Oğuzhan'}, 'logger': {'filepath': './logs/ai-agent.log', 'rotation': '50MB'}, 'agent': {'language_model_name': 'llama-3.3-70b-versatile', 'model_provider': 'groq'}, 'fastapi': {'host': '0.0.0.0', 'port': 9090, 'reload': True}}]
[2025-04-14 22:59:25] | INFO | [agent:__post_init__:20] | [post init worked successsfully!]
[2025-04-14 22:59:25] | INFO | [fastapi:run:42] | [Server Initialized!]
[2025-04-14 22:59:57] | DEBUG | [app:main:12] | [############ AI AGENT CONFIGURATIONS ############]
[2025-04-14 22:59:57] | DEBUG | [app:main:13] | [{'project': {'name': 'AI AGENT', 'description': 'Ai Agent Repository', 'company': 'Oguzhan', 'author': 'Mert Oğuzhan'}, 'logger': {'filepath': './logs/ai-agent.log', 'rotation': '50MB'}, 'agent': {'language_model_name': 'llama-3.3-70b-versatile', 'model_provider': 'groq'}, 'fastapi': {'host': '0.0.0.0', 'port': 9090, 'reload': True}}]
[2025-04-14 22:59:57] | INFO | [agent:__post_init__:20] | [post init worked successsfully!]
[2025-04-14 22:59:57] | INFO | [fastapi:run:42] | [Server Initialized!]
[2025-04-15 21:54:29] | DEBUG | [app:main:12] | [############ AI AGENT CONFIGURATIONS ############]
[2025-04-15 21:54:29] | DEBUG | [app:main:13] | [{'project': {'name': 'AI AGENT', 'description': 'Ai Agent Repository', 'company': 'Oguzhan', 'author': 'Mert Oğuzhan'}, 'logger': {'filepath': './logs/ai-agent.log', 'rotation': '50MB'}, 'agent': {'language_model_name': 'llama-3.3-70b-versatile', 'model_provider': 'groq'}, 'fastapi': {'host': '0.0.0.0', 'port': 9090, 'reload': True}}]
[2025-04-15 21:54:51] | DEBUG | [app:main:12] | [############ AI AGENT CONFIGURATIONS ############]
[2025-04-15 21:54:51] | DEBUG | [app:main:13] | [{'project': {'name': 'AI AGENT', 'description': 'Ai Agent Repository', 'company': 'Oguzhan', 'author': 'Mert Oğuzhan'}, 'logger': {'filepath': './logs/ai-agent.log', 'rotation': '50MB'}, 'agent': {'language_model_name': 'llama-3.3-70b-versatile', 'model_provider': 'groq'}, 'fastapi': {'host': '0.0.0.0', 'port': 9090, 'reload': True}}]
[2025-04-15 21:54:51] | INFO | [agent:__post_init__:20] | [post init worked successsfully!]
[2025-04-15 21:54:51] | INFO | [fastapi:run:36] | [Server Initialized!]
[2025-04-15 22:00:12] | DEBUG | [app:main:12] | [############ AI AGENT CONFIGURATIONS ############]
[2025-04-15 22:00:12] | DEBUG | [app:main:13] | [{'project': {'name': 'AI AGENT', 'description': 'Ai Agent Repository', 'company': 'Oguzhan', 'author': 'Mert Oğuzhan'}, 'logger': {'filepath': './logs/ai-agent.log', 'rotation': '50MB'}, 'agent': {'language_model_name': 'llama-3.3-70b-versatile', 'model_provider': 'groq'}, 'fastapi': {'host': '0.0.0.0', 'port': 9090, 'reload': True}}]
[2025-04-15 22:00:12] | INFO | [agent:__post_init__:20] | [post init worked successsfully!]
[2025-04-15 22:00:12] | INFO | [fastapi:run:42] | [Server Initialized!]
[2025-04-15 22:00:16] | INFO | [fastapi:request_medium_api:37] | [{'id': '3bf6e1fd2c37'}]
[2025-04-15 22:03:12] | DEBUG | [app:main:12] | [############ AI AGENT CONFIGURATIONS ############]
[2025-04-15 22:03:12] | DEBUG | [app:main:13] | [{'project': {'name': 'AI AGENT', 'description': 'Ai Agent Repository', 'company': 'Oguzhan', 'author': 'Mert Oğuzhan'}, 'logger': {'filepath': './logs/ai-agent.log', 'rotation': '50MB'}, 'agent': {'language_model_name': 'llama-3.3-70b-versatile', 'model_provider': 'groq'}, 'fastapi': {'host': '0.0.0.0', 'port': 9090, 'reload': True}}]
[2025-04-15 22:03:12] | INFO | [agent:__post_init__:20] | [post init worked successsfully!]
[2025-04-15 22:03:12] | INFO | [fastapi:run:41] | [Server Initialized!]
[2025-04-15 22:03:22] | INFO | [fastapi:request_medium_api:36] | [{'id': '3bf6e1fd2c37', 'username': 'maordinateur36', 'fullname': 'Mert Oğuzhan', 'bio': 'Computer Engineer', 'followers_count': 11, 'following_count': 22, 'publication_following_count': 5, 'image_url': 'https://miro.medium.com/v2/1*Bj3JCzVYmqtF20qrUefmhQ.png', 'twitter_username': '', 'is_writer_program_enrolled': True, 'allow_notes': True, 'medium_member_at': '', 'friend_since': '', 'is_suspended': False, 'top_writer_in': [], 'has_list': False, 'is_book_author': False, 'tipping_link': None, 'bg_image_url': '', 'logo_image_url': '', 'tier': 'FREE'}]
[2025-04-15 22:05:58] | DEBUG | [app:main:12] | [############ AI AGENT CONFIGURATIONS ############]
[2025-04-15 22:05:58] | DEBUG | [app:main:13] | [{'project': {'name': 'AI AGENT', 'description': 'Ai Agent Repository', 'company': 'Oguzhan', 'author': 'Mert Oğuzhan'}, 'logger': {'filepath': './logs/ai-agent.log', 'rotation': '50MB'}, 'agent': {'language_model_name': 'llama-3.3-70b-versatile', 'model_provider': 'groq'}, 'fastapi': {'host': '0.0.0.0', 'port': 9090, 'reload': True}}]
[2025-04-15 22:05:58] | INFO | [agent:__post_init__:20] | [post init worked successsfully!]
[2025-04-15 22:05:58] | INFO | [fastapi:run:41] | [Server Initialized!]
[2025-04-15 22:06:05] | INFO | [fastapi:request_medium_api:36] | [{'id': '3bf6e1fd2c37', 'tags_followed': ['startup', 'neural-networks', 'transformers', 'deep-learning', 'convolutional-network', 'nlp', 'software-engineering', 'cnn'], 'count': 8}]
[2025-04-15 22:14:26] | DEBUG | [app:main:12] | [############ AI AGENT CONFIGURATIONS ############]
[2025-04-15 22:14:26] | DEBUG | [app:main:13] | [{'project': {'name': 'AI AGENT', 'description': 'Ai Agent Repository', 'company': 'Oguzhan', 'author': 'Mert Oğuzhan'}, 'logger': {'filepath': './logs/ai-agent.log', 'rotation': '50MB'}, 'agent': {'language_model_name': 'llama-3.3-70b-versatile', 'model_provider': 'groq'}, 'fastapi': {'host': '0.0.0.0', 'port': 9090, 'reload': True}}]
[2025-04-15 22:14:26] | INFO | [agent:__post_init__:20] | [post init worked successsfully!]
[2025-04-15 22:14:26] | INFO | [fastapi:run:41] | [Server Initialized!]
[2025-04-15 22:14:35] | INFO | [fastapi:request_medium_api:36] | [{'id': '0f0e185bb083', 'html': '<div class="blog"><h1 id="ecef">AI Agents: Multi-Agent Architectures ( Part-7)</h1><p><i>Discover AI agents, their design, and real-world applications.</i></p><hr><h2 id="e613">Posts in this Series</h2><ol><li><i><b><a href="https://medium.com/@vipra_singh/ai-agents-introduction-part-1-fbec7edb857d" target="_blank">Introduction</a></b></i></li><li><i><b><a href="https://medium.com/@vipra_singh/ai-agents-build-an-agent-from-scratch-part-2-7ae11840c93a" target="_blank">Build Agent from Scratch</a></b></i></li><li><i><b><a href="https://medium.com/@vipra_singh/ai-agents-frameworks-part-3-ca8ce33c2f35" target="_blank">AI Agent Frameworks</a></b></i></li><li><i><b><a href="https://medium.com/@vipra_singh/ai-agent-types-part-4-a02b91bbabb4" target="_blank">Types of AI Agents</a></b></i></li><li><i><b><a href="https://medium.com/@vipra_singh/ai-agent-workflow-vs-agent-part-5-2026a890a33d" target="_blank">Workflow vs Agent</a></b></i></li><li><i><b><a href="https://medium.com/@vipra_singh/ai-agents-architectures-part-6-538812b1e17d" target="_blank">Agent Architectures</a></b></i></li><li><i><b>Multi-Agent Architectures (This Post)</b></i></li><li><i><b>Building Multi-Agent System</b></i></li><li><i><b>Short-Term and Long-Term Memory</b></i></li><li><i><b>Agentic RAG</b></i></li><li><i><b>Agentic Guardrails</b></i></li><li><i><b>Model Context Protocol (MCP)</b></i></li><li><i><b>Evaluation</b></i></li></ol><hr><h2 id="8e52">Table of Contents</h2><p><b>· <a href="#5c97" target="_blank">1. Introduction</a></b>\n ∘ <a href="#e642" target="_blank">1.1 Single-agent vs. Multi-agent Architectures</a>\n<b>· <a href="#239d" target="_blank">2. Multi-agent architectures</a>\n ∘ <a href="#2506" target="_blank">2.1 Patterns in Multi-agent Systems</a></b>\n ∘ <a href="#b6bd" target="_blank">2.1.1 Parallel</a>\n ∘ <a href="#2efc" target="_blank">2.1.2 Sequential</a>\n ∘ <a href="#7570" target="_blank">2.1.3 Loop</a>\n ∘ <a href="#646a" target="_blank">2.1.4 Router</a>\n ∘ <a href="#7153" target="_blank">2.1.5 Aggregator (or Synthesizer)</a>\n ∘ <a href="#ce5f" target="_blank">2.1.6 Network (or Horizontal)</a>\n ∘ <a href="#ad93" target="_blank">2.1.7 Handoffs</a>\n ∘ <a href="#77bb" target="_blank">2.1.8 Supervisor</a>\n ∘ <a href="#3be5" target="_blank">2.1.9 Supervisor (tool-calling)</a>\n ∘ <a href="#61d9" target="_blank">2.1.10 Hierarchical (Or Vertical)</a>\n ∘ <a href="#6059" target="_blank">2.1.11 Custom multi-agent workflow</a>\n<b>· <a href="#9c0b" target="_blank">3. Communication between agents</a></b>\n ∘ <a href="#71cf" target="_blank">3.1 Graph state vs tool calls</a>\n ∘ <a href="#0437" target="_blank">3.2 Different state schemas</a>\n ∘ <a href="#10a1" target="_blank">3.3 Shared message list</a>\n<b>· <a href="#2332" target="_blank">4. Conclusion</a></b></p><hr><h2 id="5c97">1. Introduction</h2><p>An <a href="https://langchain-ai.github.io/langgraph/concepts/agentic_concepts/#agent-architectures" target="_blank">agent</a> is <i>a system that uses an LLM to decide the control flow of an application</i>. As you develop these systems, they might grow more complex over time, making them harder to manage and scale. For example, you might run into the following problems:</p><ul><li>agent has too many tools at its disposal and makes poor decisions about which tool to call next</li><li>context grows too complex for a single agent to keep track of</li><li>there is a need for multiple specialization areas in the system (e.g. planner, researcher, math expert, etc.)</li></ul><p>To tackle these, you might consider breaking your application into multiple smaller, independent agents and composing them into a <b>multi-agent system</b>. These independent agents can be as simple as a prompt and an LLM call, or as complex as a <a href="https://langchain-ai.github.io/langgraph/concepts/agentic_concepts/#react-implementation" target="_blank">ReAct</a> agent (and more!).</p><img alt="Image by Author" src="https://miro.medium.com/1*F1fdVHE-AdqcGvt-M6KXKw.png"></img><p>By the development of agentic frameworks many companies started building their own multi-agent systems and searched for a silver bullet solution to all of their agent tasks. Two years ago researchers designed multi-agent collaboration system called <b>ChatDev</b>. ChatDev stands as a virtual software company that operates through various intelligent agents holding different roles such as chief execute officer, chief product officer, art designer, coder, reviewer, tester, etc just like a regular software engineering company.</p><img alt="Credits: ChatDev" src="https://miro.medium.com/0*b7oIbhotk5UBx1Ru.png"></img><p>All of these agents worked together and communicated with one another to create a video game, and their efforts proved successful. Following this achievement, many believed that any software engineering task could be tackled using this multi-agent architecture, where each AI has a distinct role. However, real-world experiments have shown that not every problem can be solved with this same architecture. In some cases, simpler architectures could offer more effective, cost-efficient solutions.</p><h3 id="e642">1.1 Single-agent vs. Multi-agent Architectures</h3><p>A <b>single-agent</b> approach can make sense at first (i.e., one AI agent that can do everything from navigating a browser to dealing with file operations). Over time though, as the tasks become more complex and the number of tools grow larger, our single-agent approach will start to be challenging.</p><img alt="Single-agent | Credits: Weaviate" src="https://miro.medium.com/1*r3w1FKATMcfAFbzYDgefOg.png"></img><p>We will notice effects when the agent starts to misbehave, which can result from:</p><ul><li><b>Too many tools</b>: The agent gets confused on which tools to use and/or when.</li><li><b>Too much context</b>: The agent\'s increasingly large context windows contain too many tools.</li><li><b>Too many mistakes</b>: The agent starts to produce suboptimal or incorrect results due to overly broad responsibilities.</li></ul><p>When we start automating multiple distinct subtasks such as data extraction or report generation, it might be time to start separating responsibilities. By using <b>multiple AI agents</b>, where each agent focuses on its own domain and toolkits, we can enhance the clarity and quality of our solution. Not only does this allow the agent to become more effective, but it also eases the development of the agents themselves.</p><img alt="Multi-agent | Credits: Weaviate" src="https://miro.medium.com/1*eBSZCD1rVrO1gJROQD2fKw.png"></img><h2 id="239d">2. Multi-agent architectures</h2><p>As you can see both, single-agent and multi-agent architectures have both strengths and weaknesses. Single-agent architectures are ideal when the task is straightforward and well-defined and you don\'t have specific resource constraints. On the other hand, multi-agent architectures are helpful when the use case is complex and dynamic, requires more specialized knowledge and collaboration, or has scalability and adaptability requirements.</p><h3 id="2506">2.1 Patterns in Multi-agent Systems</h3><p>There are several ways to connect agents in a multi-agent system:</p><h3 id="b6bd"><b>2.1.1 Parallel</b></h3><p>Multiple agents work simultaneously on different parts of a task.</p><img alt="Credits: Weaviate" src="https://miro.medium.com/1*RUjppjgx8OVOp6ELWBhzkw.png"></img><p><b>Example:</b> We want to <b>summarize</b>, <b>translate</b>, and <b>analyze sentiment</b> of a given text <b>simultaneously</b> using 3 agents.</p><img alt="Image by Author" src="https://miro.medium.com/1*d7EZCwOXHrhcyg-McpD5cA.png"></img><p><b>Code:</b></p><pre><code class="language-python">from typing import Dict, Any, TypedDict\nfrom langgraph.graph import StateGraph, END\nfrom langchain_core.runnables import RunnableConfig\nfrom textblob import TextBlob\nimport re\nimport time\n\n# Define the state\nclass AgentState(TypedDict):\n    text: str\n    summary: str\n    translation: str\n    sentiment: str\n    summary_time: float\n    translation_time: float\n    sentiment_time: float\n\n# Summarization Agent\ndef summarize_agent(state: AgentState) -> Dict[str, Any]:\n    print("Summarization Agent: Running")\n    start_time = time.time()\n    \n    try:\n        text = state["text"]\n        if not text.strip():\n            return {\n                "summary": "No text provided for summarization.",\n                "summary_time": 0.0\n            }\n            \n        time.sleep(2)\n        sentences = re.split(r\'(?<=[.!?]) +\', text.strip())\n        scored_sentences = [(s, len(s.split())) for s in sentences if s]\n        top_sentences = [s for s, _ in sorted(scored_sentences, key=lambda x: x[1], reverse=True)[:2]]\n        summary = " ".join(top_sentences) if top_sentences else "Text too short to summarize."\n        \n        processing_time = time.time() - start_time\n        print(f"Summarization Agent: Completed in {processing_time:.2f} seconds")\n        \n        return {\n            "summary": summary,\n            "summary_time": processing_time\n        }\n    except Exception as e:\n        return {\n            "summary": f"Error in summarization: {str(e)}",\n            "summary_time": 0.0\n        }\n\n# Translation Agent\ndef translate_agent(state: AgentState) -> Dict[str, Any]:\n    print("Translation Agent: Running")\n    start_time = time.time()\n    \n    try:\n        text = state["text"]\n        if not text.strip():\n            return {\n                "translation": "No text provided for translation.",\n                "translation_time": 0.0\n            }\n            \n        time.sleep(3)\n        translation = (\n            "El nuevo parque en la ciudad es una maravillosa adición. "\n            "Las familias disfrutan de los espacios abiertos, y a los niños les encanta el parque infantil. "\n            "Sin embargo, algunas personas piensan que el área de estacionamiento es demasiado pequeña."\n        )\n        \n        processing_time = time.time() - start_time\n        print(f"Translation Agent: Completed in {processing_time:.2f} seconds")\n        \n        return {\n            "translation": translation,\n            "translation_time": processing_time\n        }\n    except Exception as e:\n        return {\n            "translation": f"Error in translation: {str(e)}",\n            "translation_time": 0.0\n        }\n\n# Sentiment Agent\ndef sentiment_agent(state: AgentState) -> Dict[str, Any]:\n    print("Sentiment Agent: Running")\n    start_time = time.time()\n    \n    try:\n        text = state["text"]\n        if not text.strip():\n            return {\n                "sentiment": "No text provided for sentiment analysis.",\n                "sentiment_time": 0.0\n            }\n            \n        time.sleep(1.5)\n        blob = TextBlob(text)\n        polarity = blob.sentiment.polarity\n        subjectivity = blob.sentiment.subjectivity\n        \n        sentiment = "Positive" if polarity > 0 else "Negative" if polarity < 0 else "Neutral"\n        result = f"{sentiment} (Polarity: {polarity:.2f}, Subjectivity: {subjectivity:.2f})"\n        \n        processing_time = time.time() - start_time\n        print(f"Sentiment Agent: Completed in {processing_time:.2f} seconds")\n        \n        return {\n            "sentiment": result,\n            "sentiment_time": processing_time\n        }\n    except Exception as e:\n        return {\n            "sentiment": f"Error in sentiment analysis: {str(e)}",\n            "sentiment_time": 0.0\n        }\n\n# Join Node\ndef join_parallel_results(state: AgentState) -> AgentState:\n    return state\n\n# Build the Graph\ndef build_parallel_graph() -> StateGraph:\n    workflow = StateGraph(AgentState)\n    \n    # Define parallel branches\n    parallel_branches = {\n        "summarize_node": summarize_agent,\n        "translate_node": translate_agent,\n        "sentiment_node": sentiment_agent\n    }\n    \n    # Add parallel processing nodes\n    for name, agent in parallel_branches.items():\n        workflow.add_node(name, agent)\n    \n    # Add branching and joining nodes\n    workflow.add_node("branch", lambda state: state)  # Simplified branch function\n    workflow.add_node("join", join_parallel_results)\n    \n    # Set entry point\n    workflow.set_entry_point("branch")\n    \n    # Add edges for parallel execution\n    for name in parallel_branches:\n        workflow.add_edge("branch", name)\n        workflow.add_edge(name, "join")\n    \n    workflow.add_edge("join", END)\n    \n    return workflow.compile()\n\n# Main function\ndef main():\n    text = (\n        "The new park in the city is a wonderful addition. Families are enjoying the open spaces, "\n        "and children love the playground. However, some people think the parking area is too small."\n    )\n    \n    initial_state: AgentState = {\n        "text": text,\n        "summary": "",\n        "translation": "",\n        "sentiment": "",\n        "summary_time": 0.0,\n        "translation_time": 0.0,\n        "sentiment_time": 0.0\n    }\n    \n    print("\\nBuilding new graph...")\n    app = build_parallel_graph()\n    \n    print("\\nStarting parallel processing...")\n    start_time = time.time()\n    \n    config = RunnableConfig(parallel=True)\n    result = app.invoke(initial_state, config=config)\n    \n    total_time = time.time() - start_time\n    \n    print("\\n=== Parallel Task Results ===")\n    print(f"Input Text:\\n{text}\\n")\n    print(f"Summary:\\n{result[\'summary\']}\\n")\n    print(f"Translation (Spanish):\\n{result[\'translation\']}\\n")\n    print(f"Sentiment Analysis:\\n{result[\'sentiment\']}\\n")\n    \n    print("\\n=== Processing Times ===")\n    processing_times = {\n        "summary": result["summary_time"],\n        "translation": result["translation_time"],\n        "sentiment": result["sentiment_time"]\n    }\n    for agent, time_taken in processing_times.items():\n        print(f"{agent.capitalize()}: {time_taken:.2f} seconds")\n    \n    print(f"\\nTotal Wall Clock Time: {total_time:.2f} seconds")\n    print(f"Sum of Individual Processing Times: {sum(processing_times.values()):.2f} seconds")\n    print(f"Time Saved by Parallel Processing: {sum(processing_times.values()) - total_time:.2f} seconds")\n\nif __name__ == "__main__":\n    main()</code></pre><p><b>Output:</b></p><pre><code class="language-bash">Building new graph...\n\nStarting parallel processing...\nSentiment Agent: Running\nSummarization Agent: Running\nTranslation Agent: Running\nSentiment Agent: Completed in 1.50 seconds\nSummarization Agent: Completed in 2.00 seconds\nTranslation Agent: Completed in 3.00 seconds\n\n=== Parallel Task Results ===\nInput Text:\nThe new park in the city is a wonderful addition. Families are enjoying the open spaces, and children love the playground. However, some people think the parking area is too small.\n\nSummary:\nFamilies are enjoying the open spaces, and children love the playground. The new park in the city is a wonderful addition.\n\nTranslation (Spanish):\nEl nuevo parque en la ciudad es una maravillosa adición. Las familias disfrutan de los espacios abiertos, y a los niños les encanta el parque infantil. Sin embargo, algunas personas piensan que el área de estacionamiento es demasiado pequeña.\n\nSentiment Analysis:\nPositive (Polarity: 0.31, Subjectivity: 0.59)\n\n\n=== Processing Times ===\nSummary: 2.00 seconds\nTranslation: 3.00 seconds\nSentiment: 1.50 seconds\n\nTotal Wall Clock Time: 3.01 seconds\nSum of Individual Processing Times: 6.50 seconds\nTime Saved by Parallel Processing: 3.50 seconds</code></pre><ul><li><b>Parallelism</b>: The three tasks (summarization, translation, sentiment analysis) run simultaneously, reducing total processing time.</li><li><b>Independence</b>: Each agent operates on the input text independently, requiring no inter-agent communication during execution.</li><li><b>Coordination</b>: The queue ensures results are collected safely and displayed in order.</li><li><b>Realistic Use Case</b>: Summarizing, translating, and analyzing sentiment are common NLP tasks that benefit from parallel processing, especially with larger texts.</li></ul><h3 id="2efc"><b>2.1.2 Sequential</b></h3><p>Tasks are processed sequentially, where one agent\'s output becomes the input for the next.</p><img alt="Credits: Weaviate" src="https://miro.medium.com/1*YoVglwdGGxU_Ex6SMRpBYg.png"></img><p><b>Example:</b> Multi-step approvals.</p><p><b>Code:</b></p><pre><code class="language-python">from typing import Dict\nfrom langgraph.graph import StateGraph, MessagesState, END\nfrom langchain_core.runnables import RunnableConfig\nfrom langchain_core.messages import HumanMessage, AIMessage\nimport json\n\n# Agent 1: Team Lead\ndef team_lead_agent(state: MessagesState, config: RunnableConfig) -> Dict:\n    print("Agent (Team Lead): Starting review")\n    messages = state["messages"]\n    proposal = json.loads(messages[0].content)\n    title = proposal.get("title", "")\n    amount = proposal.get("amount", 0.0)\n\n    if not title or amount <= 0:\n        status = "Rejected"\n        comment = "Team Lead: Proposal rejected due to missing title or invalid amount."\n        goto = END\n    else:\n        status = "Approved by Team Lead"\n        comment = "Team Lead: Proposal is complete and approved."\n        goto = "dept_manager"\n\n    print(f"Agent (Team Lead): Review complete - {status}")\n    messages.append(AIMessage(\n        content=json.dumps({"status": status, "comment": comment}),\n        additional_kwargs={"agent": "team_lead", "goto": goto}\n    ))\n    return {"messages": messages}\n\n# Agent 2: Department Manager\ndef dept_manager_agent(state: MessagesState, config: RunnableConfig) -> Dict:\n    print("Agent (Department Manager): Starting review")\n    messages = state["messages"]\n    team_lead_msg = next((m for m in messages if m.additional_kwargs.get("agent") == "team_lead"), None)\n    proposal = json.loads(messages[0].content)\n    amount = proposal.get("amount", 0.0)\n\n    if json.loads(team_lead_msg.content)["status"] != "Approved by Team Lead":\n        status = "Rejected"\n        comment = "Department Manager: Skipped due to Team Lead rejection."\n        goto = END\n    elif amount > 100000:\n        status = "Rejected"\n        comment = "Department Manager: Budget exceeds limit."\n        goto = END\n    else:\n        status = "Approved by Department Manager"\n        comment = "Department Manager: Budget is within limits."\n        goto = "finance_director"\n\n    print(f"Agent (Department Manager): Review complete - {status}")\n    messages.append(AIMessage(\n        content=json.dumps({"status": status, "comment": comment}),\n        additional_kwargs={"agent": "dept_manager", "goto": goto}\n    ))\n    return {"messages": messages}\n\n# Agent 3: Finance Director\ndef finance_director_agent(state: MessagesState, config: RunnableConfig) -> Dict:\n    print("Agent (Finance Director): Starting review")\n    messages = state["messages"]\n    dept_msg = next((m for m in messages if m.additional_kwargs.get("agent") == "dept_manager"), None)\n    proposal = json.loads(messages[0].content)\n    amount = proposal.get("amount", 0.0)\n\n    if json.loads(dept_msg.content)["status"] != "Approved by Department Manager":\n        status = "Rejected"\n        comment = "Finance Director: Skipped due to Dept Manager rejection."\n    elif amount > 50000:\n        status = "Rejected"\n        comment = "Finance Director: Insufficient budget."\n    else:\n        status = "Approved"\n        comment = "Finance Director: Approved and feasible."\n\n    print(f"Agent (Finance Director): Review complete - {status}")\n    messages.append(AIMessage(\n        content=json.dumps({"status": status, "comment": comment}),\n        additional_kwargs={"agent": "finance_director", "goto": END}\n    ))\n    return {"messages": messages}\n\n# Routing function\ndef route_step(state: MessagesState) -> str:\n    for msg in reversed(state["messages"]):\n        goto = msg.additional_kwargs.get("goto")\n        if goto:\n            print(f"Routing: Agent {msg.additional_kwargs.get(\'agent\')} set goto to {goto}")\n            return goto\n    return END\n\n# Build LangGraph\nbuilder = StateGraph(MessagesState)\nbuilder.add_node("team_lead", team_lead_agent)\nbuilder.add_node("dept_manager", dept_manager_agent)\nbuilder.add_node("finance_director", finance_director_agent)\n\nbuilder.set_entry_point("team_lead")\n\nbuilder.add_conditional_edges("team_lead", route_step, {\n    "dept_manager": "dept_manager",\n    END: END\n})\nbuilder.add_conditional_edges("dept_manager", route_step, {\n    "finance_director": "finance_director",\n    END: END\n})\nbuilder.add_conditional_edges("finance_director", route_step, {\n    END: END\n})\n\nworkflow = builder.compile()\n\n# Main runner\ndef main():\n    initial_state = {\n        "messages": [\n            HumanMessage(\n                content=json.dumps({\n                    "title": "New Equipment Purchase",\n                    "amount": 40000.0,\n                    "department": "Engineering"\n                })\n            )\n        ]\n    }\n\n    result = workflow.invoke(initial_state)\n    messages = result["messages"]\n    proposal = json.loads(messages[0].content)\n    print("\\n=== Approval Results ===")\n    print(f"Proposal Title: {proposal[\'title\']}")\n\n    final_status = "Unknown"\n    comments = []\n    for msg in messages[1:]:\n        if isinstance(msg, AIMessage):\n            try:\n                data = json.loads(msg.content)\n                if "status" in data:\n                    final_status = data["status"]\n                if "comment" in data:\n                    comments.append(data["comment"])\n            except Exception:\n                continue\n\n    print(f"Final Status: {final_status}")\n    print("Comments:")\n    for comment in comments:\n        print(f"  - {comment}")\n\nif __name__ == "__main__":\n    main()</code></pre><p><b>Output (Amount = $40,000):</b></p><pre><code class="language-bash">Agent (Team Lead): Starting review\nAgent (Team Lead): Review complete - Approved by Team Lead\nRouting: Agent team_lead set goto to dept_manager\nAgent (Department Manager): Starting review\nAgent (Department Manager): Review complete - Approved by Department Manager\nRouting: Agent dept_manager set goto to finance_director\nAgent (Finance Director): Starting review\nAgent (Finance Director): Review complete - Approved\nRouting: Agent finance_director set goto to __end__\n\n=== Approval Results ===\nProposal Title: New Equipment Purchase\nFinal Status: Approved\nComments:\n  - Team Lead: Proposal is complete and approved.\n  - Department Manager: Budget is within limits.\n  - Finance Director: Approved and feasible.</code></pre><ul><li><b>Sequential Execution</b>:</li><li>Agents run in order: Team Lead → Department Manager → Finance Director.</li><li>If any agent rejects, the loop breaks, skipping remaining agents.</li><li>Each agent modifies the shared Proposal object, updating status and comments.</li><li><b>Coordination</b>:</li><li>Results are stored in a list, but the Proposal object carries the state between agents.</li><li>No multiprocessing is used, ensuring a single-threaded, ordered workflow.</li></ul><h3 id="7570"><b>2.1.3 Loop</b></h3><p>Agents operate in iterative cycles, continuously improving their outputs based on feedback from other agent(s).</p><img alt="Credits: Weaviate" src="https://miro.medium.com/1*lstvOqke_kp9cIMq0ba_6Q.png"></img><p><b>Example:</b> Evaluation use cases, such as code writing and code testing.</p><img alt="Image by Author" src="https://miro.medium.com/1*D2P8jiee2uuU1dmNtVvziQ.png"></img><p><b>Code:</b></p><pre><code class="language-python">from typing import Dict, Any, List\nfrom langgraph.graph import StateGraph, END\nfrom langchain_core.runnables import RunnableConfig\nimport textwrap\n\n# State to track the workflow\nclass EvaluationState(Dict[str, Any]):\n    code: str = ""\n    feedback: str = ""\n    passed: bool = False\n    iteration: int = 0\n    max_iterations: int = 3\n    history: List[Dict] = []\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.setdefault("code", "")\n        self.setdefault("feedback", "")\n        self.setdefault("passed", False)\n        self.setdefault("iteration", 0)\n        self.setdefault("max_iterations", 3)\n        self.setdefault("history", [])\n\n# Agent 1: Code Writer\ndef code_writer_agent(state: EvaluationState, config: RunnableConfig) -> Dict[str, Any]:\n    print(f"Iteration {state[\'iteration\'] + 1} - Code Writer: Generating code")\n    print(f"Iteration {state[\'iteration\'] + 1} - Code Writer: Received feedback: {state[\'feedback\']}")\n    \n    iteration = state["iteration"] + 1\n    feedback = state["feedback"]\n    \n    if iteration == 1:\n        # Initial attempt: Basic factorial with bugs (no handling for zero or negatives)\n        code = textwrap.dedent("""\n        def factorial(n):\n            result = 1\n            for i in range(1, n + 1):\n                result *= i\n            return result\n        """)\n        writer_feedback = "Initial code generated."\n    elif "factorial(0)" in feedback.lower():\n        # Fix for zero case\n        code = textwrap.dedent("""\n        def factorial(n):\n            if n == 0:\n                return 1\n            result = 1\n            for i in range(1, n + 1):\n                result *= i\n            return result\n        """)\n        writer_feedback = "Fixed handling for n=0."\n    elif "factorial(-1)" in feedback.lower() or "negative" in feedback.lower():\n        # Fix for negative input\n        code = textwrap.dedent("""\n        def factorial(n):\n            if n < 0:\n                raise ValueError("Factorial not defined for negative numbers")\n            if n == 0:\n                return 1\n            result = 1\n            for i in range(1, n + 1):\n                result *= i\n            return result\n        """)\n        writer_feedback = "Added error handling for negative inputs."\n    else:\n        code = state["code"]\n        writer_feedback = "No further improvements identified."\n\n    print(f"Iteration {iteration} - Code Writer: Code generated")\n    return {\n        "code": code,\n        "feedback": writer_feedback,\n        "iteration": iteration\n    }\n\n# Agent 2: Code Tester\ndef code_tester_agent(state: EvaluationState, config: RunnableConfig) -> Dict[str, Any]:\n    print(f"Iteration {state[\'iteration\']} - Code Tester: Testing code")\n    code = state["code"]\n    \n    try:\n        # Define test cases\n        test_cases = [\n            (0, 1),      # factorial(0) = 1\n            (1, 1),      # factorial(1) = 1\n            (5, 120),    # factorial(5) = 120\n            (-1, None),  # Should raise ValueError\n        ]\n\n        # Execute code in a safe namespace\n        namespace = {}\n        exec(code, namespace)\n        factorial = namespace.get(\'factorial\')\n        if not callable(factorial):\n            return {"passed": False, "feedback": "No factorial function found."}\n\n        feedback_parts = []\n        passed = True\n\n        # Run all test cases and collect all failures\n        for input_val, expected in test_cases:\n            try:\n                result = factorial(input_val)\n                if expected is None:  # Expecting an error\n                    passed = False\n                    feedback_parts.append(f"Test failed: factorial({input_val}) should raise an error.")\n                elif result != expected:\n                    passed = False\n                    feedback_parts.append(f"Test failed: factorial({input_val}) returned {result}, expected {expected}.")\n            except ValueError as ve:\n                if expected is not None:\n                    passed = False\n                    feedback_parts.append(f"Test failed: factorial({input_val}) raised ValueError unexpectedly: {str(ve)}")\n            except Exception as e:\n                passed = False\n                feedback_parts.append(f"Test failed: factorial({input_val}) caused error: {str(e)}")\n\n        feedback = "All tests passed!" if passed else "\\n".join(feedback_parts)\n        print(f"Iteration {state[\'iteration\']} - Code Tester: Testing complete - {\'Passed\' if passed else \'Failed\'}")\n\n        # Log the attempt in history\n        history = state["history"]\n        history.append({\n            "iteration": state["iteration"],\n            "code": code,\n            "feedback": feedback,\n            "passed": passed\n        })\n\n        return {\n            "passed": passed,\n            "feedback": feedback,\n            "history": history\n        }\n    except Exception as e:\n        print(f"Iteration {state[\'iteration\']} - Code Tester: Failed")\n        return {"passed": False, "feedback": f"Error in testing: {str(e)}"}\n\n# Conditional edge to decide whether to loop or end\ndef should_continue(state: EvaluationState) -> str:\n    if state["passed"] or state["iteration"] >= state["max_iterations"]:\n        print(f"Iteration {state[\'iteration\']} - {\'Loop stops: Tests passed\' if state[\'passed\'] else \'Loop stops: Max iterations reached\'}")\n        return "end"\n    print(f"Iteration {state[\'iteration\']} - Loop continues: Tests failed")\n    return "code_writer"\n\n# Build the LangGraph workflow\nworkflow = StateGraph(EvaluationState)\n\n# Add nodes\nworkflow.add_node("code_writer", code_writer_agent)\nworkflow.add_node("code_tester", code_tester_agent)\n\n# Add edges\nworkflow.set_entry_point("code_writer")\nworkflow.add_edge("code_writer", "code_tester")\nworkflow.add_conditional_edges(\n    "code_tester",\n    should_continue,\n    {\n        "code_writer": "code_writer",\n        "end": END\n    }\n)\n\n# Compile the graph\napp = workflow.compile()\n\n# Run the workflow\ndef main():\n    initial_state = EvaluationState()\n    result = app.invoke(initial_state)\n\n    # Display results\n    print("\\n=== Evaluation Results ===")\n    print(f"Final Status: {\'Passed\' if result[\'passed\'] else \'Failed\'} after {result[\'iteration\']} iteration(s)")\n    print(f"Final Code:\\n{result[\'code\']}")\n    print(f"Final Feedback:\\n{result[\'feedback\']}")\n    print("\\nIteration History:")\n    for attempt in result["history"]:\n        print(f"Iteration {attempt[\'iteration\']}:")\n        print(f"  Code:\\n{attempt[\'code\']}")\n        print(f"  Feedback: {attempt[\'feedback\']}")\n        print(f"  Passed: {attempt[\'passed\']}\\n")\n\nif __name__ == "__main__":\n    main()</code></pre><p><b>Output:</b></p><pre><code class="language-bash">Iteration 1 - Code Writer: Generating code\nIteration 1 - Code Writer: Received feedback: \nIteration 1 - Code Writer: Code generated\nIteration 1 - Code Tester: Testing code\nIteration 1 - Code Tester: Testing complete - Failed\nIteration 1 - Loop continues: Tests failed\nIteration 2 - Code Writer: Generating code\nIteration 2 - Code Writer: Received feedback: Test failed: factorial(-1) should raise an error.\nIteration 2 - Code Writer: Code generated\nIteration 2 - Code Tester: Testing code\nIteration 2 - Code Tester: Testing complete - Passed\nIteration 2 - Loop stops: Tests passed\n\n=== Evaluation Results ===\nFinal Status: Passed after 2 iteration(s)\nFinal Code:\n\ndef factorial(n):\n    if n < 0:\n        raise ValueError("Factorial not defined for negative numbers")\n    if n == 0:\n        return 1\n    result = 1\n    for i in range(1, n + 1):\n        result *= i\n    return result\n\nFinal Feedback:\nAll tests passed!\n\nIteration History:\nIteration 1:\n  Code:\n\ndef factorial(n):\n    result = 1\n    for i in range(1, n + 1):\n        result *= i\n    return result\n\n  Feedback: Test failed: factorial(-1) should raise an error.\n  Passed: False\n\nIteration 2:\n  Code:\n\ndef factorial(n):\n    if n < 0:\n        raise ValueError("Factorial not defined for negative numbers")\n    if n == 0:\n        return 1\n    result = 1\n    for i in range(1, n + 1):\n        result *= i\n    return result\n\n  Feedback: All tests passed!\n  Passed: True</code></pre><ul><li><b>Comprehensive Feedback</b>: The Code Tester now reports all test failures, ensuring the Code Writer has the information needed to fix issues incrementally.</li><li><b>Correct Feedback Handling</b>: The Code Writer prioritizes fixes (zero case first, then negative inputs), ensuring progressive improvement.</li><li><b>Loop Termination</b>: The loop correctly exits when tests pass, rather than running all 3 iterations unnecessarily.</li></ul><h3 id="646a"><b>2.1.4 Router</b></h3><p>A central router determines which agent(s) to invoke based on the task or input.</p><img alt="Credits: Weaviate" src="https://miro.medium.com/1*Y9ad4bbOp0645g11GBVEjg.png"></img><p><b>Example:</b> Customer Support Ticket Routing</p><img alt="Image by Author" src="https://miro.medium.com/1*i26ryT0haR9MpQ5olN1khA.png"></img><p><b>Code:</b></p><pre><code class="language-python">from typing import Dict, Any, TypedDict, Literal\nfrom langgraph.graph import StateGraph, END\nfrom langchain_core.runnables import RunnableConfig\nimport re\nimport time\n\n# Step 1: Define the State\n# The state holds the ticket information and the processing results\nclass TicketState(TypedDict):\n    ticket_text: str  # The content of the ticket\n    category: str     # The determined category (Billing, Technical, General, or Unknown)\n    resolution: str   # The resolution provided by the support team\n    processing_time: float  # Time taken to process the ticket\n\n# Step 2: Define the Router Agent\n# This agent analyzes the ticket and determines its category\ndef router_agent(state: TicketState) -> Dict[str, Any]:\n    print("Router Agent: Analyzing ticket...")\n    start_time = time.time()\n    \n    ticket_text = state["ticket_text"].lower()\n    \n    # Simple keyword-based categorization (could be replaced with an LLM or ML model)\n    if any(keyword in ticket_text for keyword in ["billing", "payment", "invoice", "charge"]):\n        category = "Billing"\n    elif any(keyword in ticket_text for keyword in ["technical", "bug", "error", "crash"]):\n        category = "Technical"\n    elif any(keyword in ticket_text for keyword in ["general", "question", "inquiry", "info"]):\n        category = "General"\n    else:\n        category = "Unknown"\n    \n    processing_time = time.time() - start_time\n    print(f"Router Agent: Categorized as \'{category}\' in {processing_time:.2f} seconds")\n    \n    return {\n        "category": category,\n        "processing_time": processing_time\n    }\n\n# Step 3: Define the Support Team Agents\n# Each agent handles tickets for a specific category\n\n# Billing Team Agent\ndef billing_team_agent(state: TicketState) -> Dict[str, Any]:\n    print("Billing Team Agent: Processing ticket...")\n    start_time = time.time()\n    \n    ticket_text = state["ticket_text"]\n    resolution = f"Billing Team: Reviewed ticket \'{ticket_text}\'. Please check your invoice details or contact our billing department for further assistance."\n    \n    processing_time = time.time() - start_time\n    time.sleep(1)  # Simulate processing time\n    print(f"Billing Team Agent: Completed in {processing_time:.2f} seconds")\n    \n    return {\n        "resolution": resolution,\n        "processing_time": state["processing_time"] + processing_time\n    }\n\n# Technical Support Team Agent\ndef technical_team_agent(state: TicketState) -> Dict[str, Any]:\n    print("Technical Team Agent: Processing ticket...")\n    start_time = time.time()\n    \n    ticket_text = state["ticket_text"]\n    resolution = f"Technical Team: Reviewed ticket \'{ticket_text}\'. Please try restarting your device or submit a detailed error log for further investigation."\n    \n    processing_time = time.time() - start_time\n    time.sleep(1.5)  # Simulate processing time\n    print(f"Technical Team Agent: Completed in {processing_time:.2f} seconds")\n    \n    return {\n        "resolution": resolution,\n        "processing_time": state["processing_time"] + processing_time\n    }\n\n# General Support Team Agent\ndef general_team_agent(state: TicketState) -> Dict[str, Any]:\n    print("General Team Agent: Processing ticket...")\n    start_time = time.time()\n    \n    ticket_text = state["ticket_text"]\n    resolution = f"General Team: Reviewed ticket \'{ticket_text}\'. For more information, please refer to our FAQ or contact us via email."\n    \n    processing_time = time.time() - start_time\n    time.sleep(0.8)  # Simulate processing time\n    print(f"General Team Agent: Completed in {processing_time:.2f} seconds")\n    \n    return {\n        "resolution": resolution,\n        "processing_time": state["processing_time"] + processing_time\n    }\n\n# Manual Review Agent (for unknown categories)\ndef manual_review_agent(state: TicketState) -> Dict[str, Any]:\n    print("Manual Review Agent: Processing ticket...")\n    start_time = time.time()\n    \n    ticket_text = state["ticket_text"]\n    resolution = f"Manual Review: Ticket \'{ticket_text}\' could not be categorized. Flagged for human review. Please assign to the appropriate team manually."\n    \n    processing_time = time.time() - start_time\n    time.sleep(0.5)  # Simulate processing time\n    print(f"Manual Review Agent: Completed in {processing_time:.2f} seconds")\n    \n    return {\n        "resolution": resolution,\n        "processing_time": state["processing_time"] + processing_time\n    }\n\n# Step 4: Define the Router Function\n# This function determines the next node based on the ticket category\ndef route_ticket(state: TicketState) -> Literal["billing_team", "technical_team", "general_team", "manual_review"]:\n    category = state["category"]\n    print(f"Routing: Ticket category is \'{category}\'")\n    \n    if category == "Billing":\n        return "billing_team"\n    elif category == "Technical":\n        return "technical_team"\n    elif category == "General":\n        return "general_team"\n    else:\n        return "manual_review"\n\n# Step 5: Build the Graph with a Router Pattern\ndef build_router_graph() -> StateGraph:\n    workflow = StateGraph(TicketState)\n    \n    # Add nodes\n    workflow.add_node("router", router_agent)  # Entry point: Categorizes the ticket\n    workflow.add_node("billing_team", billing_team_agent)  # Handles billing tickets\n    workflow.add_node("technical_team", technical_team_agent)  # Handles technical tickets\n    workflow.add_node("general_team", general_team_agent)  # Handles general inquiries\n    workflow.add_node("manual_review", manual_review_agent)  # Handles uncategorized tickets\n    \n    # Set the entry point\n    workflow.set_entry_point("router")\n    \n    # Add conditional edges for routing\n    workflow.add_conditional_edges(\n        "router",\n        route_ticket,  # Router function to determine the next node\n        {\n            "billing_team": "billing_team",\n            "technical_team": "technical_team",\n            "general_team": "general_team",\n            "manual_review": "manual_review"\n        }\n    )\n    \n    # Add edges from each team to END\n    workflow.add_edge("billing_team", END)\n    workflow.add_edge("technical_team", END)\n    workflow.add_edge("general_team", END)\n    workflow.add_edge("manual_review", END)\n    \n    return workflow.compile()\n\n# Step 6: Run the Workflow\ndef main():\n    # Test cases for different ticket categories\n    test_tickets = [\n        "I have a billing issue with my last invoice. It seems I was overcharged.",\n        "My app keeps crashing with a technical error. Please help!",\n        "I have a general question about your services. Can you provide more info?",\n        "I need assistance with something unrelated to billing or technical issues."\n    ]\n    \n    for ticket_text in test_tickets:\n        # Initialize the state for each ticket\n        initial_state: TicketState = {\n            "ticket_text": ticket_text,\n            "category": "",\n            "resolution": "",\n            "processing_time": 0.0\n        }\n        \n        print(f"\\n=== Processing Ticket: \'{ticket_text}\' ===")\n        app = build_router_graph()\n        \n        start_time = time.time()\n        result = app.invoke(initial_state, config=RunnableConfig())\n        total_time = time.time() - start_time\n        \n        print("\\n=== Ticket Results ===")\n        print(f"Category: {result[\'category\']}")\n        print(f"Resolution: {result[\'resolution\']}")\n        print(f"Total Processing Time: {result[\'processing_time\']:.2f} seconds")\n        print(f"Total Wall Clock Time: {total_time:.2f} seconds")\n        print("-" * 50)\n\nif __name__ == "__main__":\n    main()</code></pre><p><b>Output:</b></p><pre><code class="language-bash">=== Processing Ticket: \'I have a billing issue with my last invoice. It seems I was overcharged.\' ===\nRouter Agent: Analyzing ticket...\nRouter Agent: Categorized as \'Billing\' in 0.00 seconds\nRouting: Ticket category is \'Billing\'\nBilling Team Agent: Processing ticket...\nBilling Team Agent: Completed in 0.00 seconds\n\n=== Ticket Results ===\nCategory: Billing\nResolution: Billing Team: Reviewed ticket \'I have a billing issue with my last invoice. It seems I was overcharged.\'. Please check your invoice details or contact our billing department for further assistance.\nTotal Processing Time: 0.00 seconds\nTotal Wall Clock Time: 1.03 seconds\n--------------------------------------------------\n\n=== Processing Ticket: \'My app keeps crashing with a technical error. Please help!\' ===\nRouter Agent: Analyzing ticket...\nRouter Agent: Categorized as \'Technical\' in 0.00 seconds\nRouting: Ticket category is \'Technical\'\nTechnical Team Agent: Processing ticket...\nTechnical Team Agent: Completed in 0.00 seconds\n\n=== Ticket Results ===\nCategory: Technical\nResolution: Technical Team: Reviewed ticket \'My app keeps crashing with a technical error. Please help!\'. Please try restarting your device or submit a detailed error log for further investigation.\nTotal Processing Time: 0.00 seconds\nTotal Wall Clock Time: 1.50 seconds\n--------------------------------------------------\n\n=== Processing Ticket: \'I have a general question about your services. Can you provide more info?\' ===\nRouter Agent: Analyzing ticket...\nRouter Agent: Categorized as \'General\' in 0.00 seconds\nRouting: Ticket category is \'General\'\nGeneral Team Agent: Processing ticket...\nGeneral Team Agent: Completed in 0.00 seconds\n\n=== Ticket Results ===\nCategory: General\nResolution: General Team: Reviewed ticket \'I have a general question about your services. Can you provide more info?\'. For more information, please refer to our FAQ or contact us via email.\nTotal Processing Time: 0.00 seconds\nTotal Wall Clock Time: 0.80 seconds\n--------------------------------------------------\n\n=== Processing Ticket: \'I need assistance with something unrelated to billing or technical issues.\' ===\nRouter Agent: Analyzing ticket...\nRouter Agent: Categorized as \'Billing\' in 0.00 seconds\nRouting: Ticket category is \'Billing\'\nBilling Team Agent: Processing ticket...\nBilling Team Agent: Completed in 0.00 seconds\n\n=== Ticket Results ===\nCategory: Billing\nResolution: Billing Team: Reviewed ticket \'I need assistance with something unrelated to billing or technical issues.\'. Please check your invoice details or contact our billing department for further assistance.\nTotal Processing Time: 0.00 seconds\nTotal Wall Clock Time: 1.00 seconds\n--------------------------------------------------</code></pre><ul><li><b>Dynamic Routing</b>: The router_agent determines the ticket category, and the route_ticket function directs the workflow to the appropriate node using add_conditional_edges.</li><li><b>Condition-Based Flow</b>: Unlike the parallel pattern (where multiple nodes run concurrently), the router pattern executes only one path based on the condition (category).</li><li><b>Scalability</b>: You can add more support teams by extending the nodes and updating the route_ticket function to handle new categories.</li></ul><h3 id="7153"><b>2.1.5 Aggregator (or Synthesizer)</b></h3><p>Agents contribute outputs that are collected and synthesized by an aggregator agent into a final result.</p><img alt="Credits: Weaviate" src="https://miro.medium.com/1*I_9A98u0i5fX1A3a0ffSEQ.png"></img><p><b>Example:</b> Social Media Sentiment Analysis Aggregator</p><img alt="Image by Author" src="https://miro.medium.com/1*vif8t0K8I0-CYPNWg0yvEw.png"></img><p><b>Code:</b></p><pre><code class="language-python">from typing import Dict, Any, TypedDict, List\nfrom langgraph.graph import StateGraph, END\nfrom langchain_core.runnables import RunnableConfig\nfrom textblob import TextBlob\nimport time\nfrom typing_extensions import Annotated\nfrom operator import add\n\n# Step 1: Define the State\nclass SocialMediaState(TypedDict):\n    twitter_posts: List[str]\n    instagram_posts: List[str]\n    reddit_posts: List[str]\n    twitter_sentiment: Dict[str, float]\n    instagram_sentiment: Dict[str, float]\n    reddit_sentiment: Dict[str, float]\n    final_report: str\n    processing_time: Annotated[float, add]\n\n# Step 2: Define the Post Collection Agents\ndef collect_twitter_posts(state: SocialMediaState) -> Dict[str, Any]:\n    print("Twitter Agent: Collecting posts...")\n    start_time = time.time()\n    \n    posts = [\n        "Loving the new product from this brand! Amazing quality.",\n        "Terrible customer service from this brand. Very disappointed."\n    ]\n    \n    time.sleep(1)  # Simulate processing time\n    processing_time = time.time() - start_time  # Include time.sleep in processing_time\n    print(f"Twitter Agent: Completed in {processing_time:.2f} seconds")\n    \n    return {\n        "twitter_posts": posts,\n        "processing_time": processing_time\n    }\n\ndef collect_instagram_posts(state: SocialMediaState) -> Dict[str, Any]:\n    print("Instagram Agent: Collecting posts...")\n    start_time = time.time()\n    \n    posts = [\n        "Beautiful design by this brand! #loveit",\n        "Not impressed with the latest release. Expected better."\n    ]\n    \n    time.sleep(1.2)  # Simulate processing time\n    processing_time = time.time() - start_time\n    print(f"Instagram Agent: Completed in {processing_time:.2f} seconds")\n    \n    return {\n        "instagram_posts": posts,\n        "processing_time": processing_time\n    }\n\ndef collect_reddit_posts(state: SocialMediaState) -> Dict[str, Any]:\n    print("Reddit Agent: Collecting posts...")\n    start_time = time.time()\n    \n    posts = [\n        "This brand is awesome! Great value for money.",\n        "Had a bad experience with their support team. Not happy."\n    ]\n    \n    time.sleep(0.8)  # Simulate processing time\n    processing_time = time.time() - start_time\n    print(f"Reddit Agent: Completed in {processing_time:.2f} seconds")\n    \n    return {\n        "reddit_posts": posts,\n        "processing_time": processing_time\n    }\n\n# Step 3: Define the Sentiment Analysis Agents\ndef analyze_twitter_sentiment(state: SocialMediaState) -> Dict[str, Any]:\n    print("Twitter Sentiment Agent: Analyzing sentiment...")\n    start_time = time.time()\n    \n    posts = state["twitter_posts"]\n    polarities = [TextBlob(post).sentiment.polarity for post in posts]\n    avg_polarity = sum(polarities) / len(polarities) if polarities else 0.0\n    \n    time.sleep(0.5)  # Simulate processing time\n    processing_time = time.time() - start_time\n    print(f"Twitter Sentiment Agent: Completed in {processing_time:.2f} seconds")\n    \n    return {\n        "twitter_sentiment": {"average_polarity": avg_polarity, "num_posts": len(posts)},\n        "processing_time": processing_time\n    }\n\ndef analyze_instagram_sentiment(state: SocialMediaState) -> Dict[str, Any]:\n    print("Instagram Sentiment Agent: Analyzing sentiment...")\n    start_time = time.time()\n    \n    posts = state["instagram_posts"]\n    polarities = [TextBlob(post).sentiment.polarity for post in posts]\n    avg_polarity = sum(polarities) / len(polarities) if polarities else 0.0\n    \n    time.sleep(0.6)  # Simulate processing time\n    processing_time = time.time() - start_time\n    print(f"Instagram Sentiment Agent: Completed in {processing_time:.2f} seconds")\n    \n    return {\n        "instagram_sentiment": {"average_polarity": avg_polarity, "num_posts": len(posts)},\n        "processing_time": processing_time\n    }\n\ndef analyze_reddit_sentiment(state: SocialMediaState) -> Dict[str, Any]:\n    print("Reddit Sentiment Agent: Analyzing sentiment...")\n    start_time = time.time()\n    \n    posts = state["reddit_posts"]\n    polarities = [TextBlob(post).sentiment.polarity for post in posts]\n    avg_polarity = sum(polarities) / len(polarities) if polarities else 0.0\n    \n    time.sleep(0.4)  # Simulate processing time\n    processing_time = time.time() - start_time\n    print(f"Reddit Sentiment Agent: Completed in {processing_time:.2f} seconds")\n    \n    return {\n        "reddit_sentiment": {"average_polarity": avg_polarity, "num_posts": len(posts)},\n        "processing_time": processing_time\n    }\n\n# Step 4: Define the Aggregator Agent\ndef aggregate_results(state: SocialMediaState) -> Dict[str, Any]:\n    print("Aggregator Agent: Generating final report...")\n    start_time = time.time()\n    \n    twitter_sentiment = state["twitter_sentiment"]\n    instagram_sentiment = state["instagram_sentiment"]\n    reddit_sentiment = state["reddit_sentiment"]\n    \n    total_posts = (twitter_sentiment["num_posts"] +\n                   instagram_sentiment["num_posts"] +\n                   reddit_sentiment["num_posts"])\n    weighted_polarity = (\n        twitter_sentiment["average_polarity"] * twitter_sentiment["num_posts"] +\n        instagram_sentiment["average_polarity"] * instagram_sentiment["num_posts"] +\n        reddit_sentiment["average_polarity"] * reddit_sentiment["num_posts"]\n    ) / total_posts if total_posts > 0 else 0.0\n    \n    overall_sentiment = ("Positive" if weighted_polarity > 0 else\n                         "Negative" if weighted_polarity < 0 else "Neutral")\n    \n    report = (\n        f"Overall Sentiment: {overall_sentiment} (Average Polarity: {weighted_polarity:.2f})\\n"\n        f"Twitter Sentiment: {twitter_sentiment[\'average_polarity\']:.2f} (Posts: {twitter_sentiment[\'num_posts\']})\\n"\n        f"Instagram Sentiment: {instagram_sentiment[\'average_polarity\']:.2f} (Posts: {instagram_sentiment[\'num_posts\']})\\n"\n        f"Reddit Sentiment: {reddit_sentiment[\'average_polarity\']:.2f} (Posts: {reddit_sentiment[\'num_posts\']})"\n    )\n    \n    time.sleep(0.3)  # Simulate processing time\n    processing_time = time.time() - start_time\n    print(f"Aggregator Agent: Completed in {processing_time:.2f} seconds")\n    \n    return {\n        "final_report": report,\n        "processing_time": processing_time\n    }\n\n# Step 5: Build the Graph with an Aggregator Pattern\ndef build_aggregator_graph() -> StateGraph:\n    workflow = StateGraph(SocialMediaState)\n    \n    # Add nodes for collecting posts\n    workflow.add_node("collect_twitter", collect_twitter_posts)\n    workflow.add_node("collect_instagram", collect_instagram_posts)\n    workflow.add_node("collect_reddit", collect_reddit_posts)\n    \n    # Add nodes for sentiment analysis\n    workflow.add_node("analyze_twitter", analyze_twitter_sentiment)\n    workflow.add_node("analyze_instagram", analyze_instagram_sentiment)\n    workflow.add_node("analyze_reddit", analyze_reddit_sentiment)\n    \n    # Add node for aggregation\n    workflow.add_node("aggregate", aggregate_results)\n    \n    # Add a branching node to trigger all collection nodes in parallel\n    workflow.add_node("branch", lambda state: state)\n    \n    # Set the entry point to the branch node\n    workflow.set_entry_point("branch")\n    \n    # Add edges from branch to collection nodes (parallel execution)\n    workflow.add_edge("branch", "collect_twitter")\n    workflow.add_edge("branch", "collect_instagram")\n    workflow.add_edge("branch", "collect_reddit")\n    \n    # Add edges from collection to sentiment analysis\n    workflow.add_edge("collect_twitter", "analyze_twitter")\n    workflow.add_edge("collect_instagram", "analyze_instagram")\n    workflow.add_edge("collect_reddit", "analyze_reddit")\n    \n    # Add edges from sentiment analysis to aggregator\n    workflow.add_edge("analyze_twitter", "aggregate")\n    workflow.add_edge("analyze_instagram", "aggregate")\n    workflow.add_edge("analyze_reddit", "aggregate")\n    \n    # Add edge from aggregator to END\n    workflow.add_edge("aggregate", END)\n    \n    return workflow.compile()\n\n# Step 6: Run the Workflow\ndef main():\n    initial_state: SocialMediaState = {\n        "twitter_posts": [],\n        "instagram_posts": [],\n        "reddit_posts": [],\n        "twitter_sentiment": {"average_polarity": 0.0, "num_posts": 0},\n        "instagram_sentiment": {"average_polarity": 0.0, "num_posts": 0},\n        "reddit_sentiment": {"average_polarity": 0.0, "num_posts": 0},\n        "final_report": "",\n        "processing_time": 0.0\n    }\n    \n    print("\\nStarting social media sentiment analysis...")\n    app = build_aggregator_graph()\n    \n    start_time = time.time()\n    config = RunnableConfig(parallel=True)\n    result = app.invoke(initial_state, config=config)\n    total_time = time.time() - start_time\n    \n    print("\\n=== Sentiment Analysis Results ===")\n    print(result["final_report"])\n    print(f"\\nTotal Processing Time: {result[\'processing_time\']:.2f} seconds")\n    print(f"Total Wall Clock Time: {total_time:.2f} seconds")\n\nif __name__ == "__main__":\n    main()</code></pre><p><b>Output:</b></p><pre><code class="language-bash">Starting social media sentiment analysis...\nInstagram Agent: Collecting posts...\nReddit Agent: Collecting posts...\nTwitter Agent: Collecting posts...\nReddit Agent: Completed in 0.80 seconds\nTwitter Agent: Completed in 1.00 seconds\nInstagram Agent: Completed in 1.20 seconds\nInstagram Sentiment Agent: Analyzing sentiment...\nReddit Sentiment Agent: Analyzing sentiment...\nTwitter Sentiment Agent: Analyzing sentiment...\nReddit Sentiment Agent: Completed in 0.40 seconds\nTwitter Sentiment Agent: Completed in 0.50 seconds\nInstagram Sentiment Agent: Completed in 0.60 seconds\nAggregator Agent: Generating final report...\nAggregator Agent: Completed in 0.30 seconds\n\n=== Sentiment Analysis Results ===\nOverall Sentiment: Positive (Average Polarity: 0.15)\nTwitter Sentiment: -0.27 (Posts: 2)\nInstagram Sentiment: 0.55 (Posts: 2)\nReddit Sentiment: 0.18 (Posts: 2)\n\nTotal Processing Time: 4.80 seconds\nTotal Wall Clock Time: 2.13 seconds</code></pre><ul><li><b>Parallel Execution</b>: The collection and analysis nodes run in parallel, reducing the total wall clock time (2.1 seconds) compared to the sum of individual processing times (3.8 seconds).</li><li><b>Aggregation</b>: The aggregate node combines the sentiment results into a final report, calculating the overall sentiment and providing a breakdown by platform.</li></ul><h3 id="ce5f"><b>2.1.6 Network (or Horizontal)</b></h3><p>Agents communicate directly with one another in a many-to-many fashion, forming a decentralized network.</p><img alt="Credits: Weaviate" src="https://miro.medium.com/1*obLJLFQpjfcrz5hqHes2tQ.png"></img><p>This architecture is good for problems that do not have a clear hierarchy of agents or a specific sequence in which agents should be called.</p><pre><code class="language-python">from typing import Literal\nfrom langchain_openai import ChatOpenAI\nfrom langgraph.graph import StateGraph, MessagesState, START, END\nmodel = ChatOpenAI()\ndef agent_1(state: MessagesState) -> Command[Literal["agent_2", "agent_3", END]]:\n    # you can pass relevant parts of the state to the LLM (e.g., state["messages"])\n    # to determine which agent to call next. a common pattern is to call the model\n    # with a structured output (e.g. force it to return an output with a "next_agent" field)\n    response = model.invoke(...)\n    # route to one of the agents or exit based on the LLM\'s decision\n    # if the LLM returns "__end__", the graph will finish execution\n    return Command(\n        goto=response["next_agent"],\n        update={"messages": [response["content"]]},\n    )\ndef agent_2(state: MessagesState) -> Command[Literal["agent_1", "agent_3", END]]:\n    response = model.invoke(...)\n    return Command(\n        goto=response["next_agent"],\n        update={"messages": [response["content"]]},\n    )\ndef agent_3(state: MessagesState) -> Command[Literal["agent_1", "agent_2", END]]:\n    ...\n    return Command(\n        goto=response["next_agent"],\n        update={"messages": [response["content"]]},\n    )\nbuilder = StateGraph(MessagesState)\nbuilder.add_node(agent_1)\nbuilder.add_node(agent_2)\nbuilder.add_node(agent_3)\nbuilder.add_edge(START, "agent_1")\nnetwork = builder.compile()</code></pre><p><b>Pros:</b> Distributed collaboration and group-driven decision-making. The system remains functional even if some agents fail.</p><p><b>Cons:</b> Managing communication among agents can become challenging. More communication may cause inefficiencies and the possibility of agents duplicating efforts.</p><h3 id="ad93">2.1.7 Handoffs</h3><p>In multi-agent architectures, agents can be represented as graph nodes. Each agent node executes its step(s) and decides whether to finish execution or route to another agent, including potentially routing to itself (e.g., running in a loop). A common pattern in multi-agent interactions is handoffs, where one agent hands off control to another. Handoffs allow you to specify:</p><ul><li><b>destination</b>: target agent to navigate to (e.g., name of the node to go to)</li><li><b>payload</b>: <a href="https://langchain-ai.github.io/langgraph/concepts/multi_agent/#communication-between-agents" target="_blank">information to pass to that agent</a> (e.g., state update)</li></ul><img alt="Credits: aka.ms/ai-agents-beginners" src="https://miro.medium.com/1*5lonb7zzgelDjT4faKPj4g.png"></img><p>To implement handoffs in LangGraph, agent nodes can return <code><a href="https://langchain-ai.github.io/langgraph/concepts/low_level/#command" target="_blank">Command</a></code> object that allows you to combine both control flow and state updates:</p><pre><code class="language-python">def agent(state) -> Command[Literal["agent", "another_agent"]]:\n    # the condition for routing/halting can be anything, e.g. LLM tool call / structured output, etc.\n    goto = get_next_agent(...)  # \'agent\' / \'another_agent\'\n    return Command(\n        # Specify which agent to call next\n        goto=goto,\n        # Update the graph state\n        update={"my_state_key": "my_state_value"}\n    )</code></pre><p>In a more complex scenario where each agent node is itself a graph (i.e., a <a href="https://langchain-ai.github.io/langgraph/concepts/low_level/#subgraphs" target="_blank">subgraph</a>), a node in one of the agent subgraphs might want to navigate to a different agent. For example, if you have two agents, <code>alice</code> and <code>bob</code> (subgraph nodes in a parent graph), and <code>alice</code> needs to navigate to <code>bob</code>, you can set <code>graph=Command.PARENT</code> in the <code>Command</code> object:</p><pre><code class="language-python">def some_node_inside_alice(state)\n    return Command(\n        goto="bob",\n        update={"my_state_key": "my_state_value"},\n        # specify which graph to navigate to (defaults to the current graph)\n        graph=Command.PARENT,\n    )</code></pre><p><b>Note</b></p><p>If you need to support visualization for subgraphs communicating using <code>Command(graph=Command.PARENT)</code> you would need to wrap them in a node function with <code>Command</code> annotation, e.g. instead of this:</p><pre><code class="language-python">builder.add_node(alice)</code></pre><p>you would need to do this:</p><pre><code class="language-python">def call_alice(state) -> Command[Literal["bob"]]:\n    return alice.invoke(state)\n\nbuilder.add_node("alice", call_alice)</code></pre><p><b>Handoffs as tools</b></p><p>One of the most common agent types is a ReAct-style tool-calling agents. For those types of agents, a common pattern is wrapping a handoff in a tool call, e.g.:</p><pre><code class="language-python">def transfer_to_bob(state):\n    """Transfer to bob."""\n    return Command(\n        goto="bob",\n        update={"my_state_key": "my_state_value"},\n        graph=Command.PARENT,\n    )</code></pre><p>This is a special case of updating the graph state from tools where in addition the state update, the control flow is included as well.</p><p><b>Important</b></p><p>If you want to use tools that return <code>Command</code>, you can either use prebuilt <code><a href="https://langchain-ai.github.io/langgraph/reference/prebuilt/#langgraph.prebuilt.chat_agent_executor.create_react_agent" target="_blank">create_react_agent</a></code> / <code><a href="https://langchain-ai.github.io/langgraph/reference/prebuilt/#langgraph.prebuilt.tool_node.ToolNode" target="_blank">ToolNode</a></code> components, or implement your own tool-executing node that collects <code>Command</code> objects returned by the tools and returns a list of them, e.g.:</p><pre><code class="language-python">def call_tools(state):\n    ...\n    commands = [tools_by_name[tool_call["name"]].invoke(tool_call) for tool_call in tool_calls]\n    return commands</code></pre><p>Let\'s now take a closer look at the different multi-agent architectures.</p><h3 id="77bb">2.1.8 Supervisor</h3><p>In this architecture, we define agents as nodes and add a supervisor node (LLM) that decides which agent nodes should be called next. We use <code><a href="https://langchain-ai.github.io/langgraph/concepts/low_level/#command" target="_blank">Command</a></code> to route execution to the appropriate agent node based on supervisor\'s decision. This architecture also lends itself well to running multiple agents in parallel or using <a href="https://langchain-ai.github.io/langgraph/how-tos/map-reduce/" target="_blank">map-reduce</a> pattern.</p><pre><code class="language-python">from typing import Literal\nfrom langchain_openai import ChatOpenAI\nfrom langgraph.graph import StateGraph, MessagesState, START, END\n\nmodel = ChatOpenAI()\n\ndef supervisor(state: MessagesState) -> Command[Literal["agent_1", "agent_2", END]]:\n    # you can pass relevant parts of the state to the LLM (e.g., state["messages"])\n    # to determine which agent to call next. a common pattern is to call the model\n    # with a structured output (e.g. force it to return an output with a "next_agent" field)\n    response = model.invoke(...)\n    # route to one of the agents or exit based on the supervisor\'s decision\n    # if the supervisor returns "__end__", the graph will finish execution\n    return Command(goto=response["next_agent"])\n\ndef agent_1(state: MessagesState) -> Command[Literal["supervisor"]]:\n    # you can pass relevant parts of the state to the LLM (e.g., state["messages"])\n    # and add any additional logic (different models, custom prompts, structured output, etc.)\n    response = model.invoke(...)\n    return Command(\n        goto="supervisor",\n        update={"messages": [response]},\n    )\n\ndef agent_2(state: MessagesState) -> Command[Literal["supervisor"]]:\n    response = model.invoke(...)\n    return Command(\n        goto="supervisor",\n        update={"messages": [response]},\n    )\n\nbuilder = StateGraph(MessagesState)\nbuilder.add_node(supervisor)\nbuilder.add_node(agent_1)\nbuilder.add_node(agent_2)\n\nbuilder.add_edge(START, "supervisor")\n\nsupervisor = builder.compile()</code></pre><h3 id="3be5">2.1.9 Supervisor (tool-calling)</h3><p>In this variant of the <a href="https://langchain-ai.github.io/langgraph/concepts/multi_agent/#supervisor" target="_blank">supervisor</a> architecture, we define individual agents as <b>tools</b> and use a tool-calling LLM in the supervisor node. This can be implemented as a <a href="https://langchain-ai.github.io/langgraph/concepts/agentic_concepts/#react-implementation" target="_blank">ReAct</a>-style agent with two nodes - an LLM node (supervisor) and a tool-calling node that executes tools (agents in this case).</p><pre><code class="language-python">from typing import Annotated\nfrom langchain_openai import ChatOpenAI\nfrom langgraph.prebuilt import InjectedState, create_react_agent\n\nmodel = ChatOpenAI()\n\n# this is the agent function that will be called as tool\n# notice that you can pass the state to the tool via InjectedState annotation\ndef agent_1(state: Annotated[dict, InjectedState]):\n    # you can pass relevant parts of the state to the LLM (e.g., state["messages"])\n    # and add any additional logic (different models, custom prompts, structured output, etc.)\n    response = model.invoke(...)\n    # return the LLM response as a string (expected tool response format)\n    # this will be automatically turned to ToolMessage\n    # by the prebuilt create_react_agent (supervisor)\n    return response.content\n\ndef agent_2(state: Annotated[dict, InjectedState]):\n    response = model.invoke(...)\n    return response.content\n\ntools = [agent_1, agent_2]\n# the simplest way to build a supervisor w/ tool-calling is to use prebuilt ReAct agent graph\n# that consists of a tool-calling LLM node (i.e. supervisor) and a tool-executing node\nsupervisor = create_react_agent(model, tools)</code></pre><h3 id="61d9">2.1.10 Hierarchical (Or Vertical)</h3><p>Agents are organized in a tree-like structure, with higher-level agents (supervisor agents) managing lower-level ones.</p><img alt="Credits: Weaviate" src="https://miro.medium.com/1*qCkNMTQC29A7_JdUIHzFtg.png"></img><p>As you add more agents to your system, it might become too hard for the supervisor to manage all of them. The supervisor might start making poor decisions about which agent to call next, the context might become too complex for a single supervisor to keep track of. In other words, you end up with the same problems that motivated the multi-agent architecture in the first place.</p><p>To address this, you can design your system <i>hierarchically</i>. For example, you can create separate, specialized teams of agents managed by individual supervisors, and a top-level supervisor to manage the teams.</p><pre><code class="language-python">from typing import Literal\nfrom langchain_openai import ChatOpenAI\nfrom langgraph.graph import StateGraph, MessagesState, START, END\nfrom langgraph.types import Command\nmodel = ChatOpenAI()\n\n# define team 1 (same as the single supervisor example above)\n\ndef team_1_supervisor(state: MessagesState) -> Command[Literal["team_1_agent_1", "team_1_agent_2", END]]:\n    response = model.invoke(...)\n    return Command(goto=response["next_agent"])\n\ndef team_1_agent_1(state: MessagesState) -> Command[Literal["team_1_supervisor"]]:\n    response = model.invoke(...)\n    return Command(goto="team_1_supervisor", update={"messages": [response]})\n\ndef team_1_agent_2(state: MessagesState) -> Command[Literal["team_1_supervisor"]]:\n    response = model.invoke(...)\n    return Command(goto="team_1_supervisor", update={"messages": [response]})\n\nteam_1_builder = StateGraph(Team1State)\nteam_1_builder.add_node(team_1_supervisor)\nteam_1_builder.add_node(team_1_agent_1)\nteam_1_builder.add_node(team_1_agent_2)\nteam_1_builder.add_edge(START, "team_1_supervisor")\nteam_1_graph = team_1_builder.compile()\n\n# define team 2 (same as the single supervisor example above)\nclass Team2State(MessagesState):\n    next: Literal["team_2_agent_1", "team_2_agent_2", "__end__"]\n\ndef team_2_supervisor(state: Team2State):\n    ...\n\ndef team_2_agent_1(state: Team2State):\n    ...\n\ndef team_2_agent_2(state: Team2State):\n    ...\n\nteam_2_builder = StateGraph(Team2State)\n...\nteam_2_graph = team_2_builder.compile()\n\n\n# define top-level supervisor\n\nbuilder = StateGraph(MessagesState)\ndef top_level_supervisor(state: MessagesState) -> Command[Literal["team_1_graph", "team_2_graph", END]]:\n    # you can pass relevant parts of the state to the LLM (e.g., state["messages"])\n    # to determine which team to call next. a common pattern is to call the model\n    # with a structured output (e.g. force it to return an output with a "next_team" field)\n    response = model.invoke(...)\n    # route to one of the teams or exit based on the supervisor\'s decision\n    # if the supervisor returns "__end__", the graph will finish execution\n    return Command(goto=response["next_team"])\n\nbuilder = StateGraph(MessagesState)\nbuilder.add_node(top_level_supervisor)\nbuilder.add_node("team_1_graph", team_1_graph)\nbuilder.add_node("team_2_graph", team_2_graph)\nbuilder.add_edge(START, "top_level_supervisor")\nbuilder.add_edge("team_1_graph", "top_level_supervisor")\nbuilder.add_edge("team_2_graph", "top_level_supervisor")\ngraph = builder.compile()</code></pre><p><b>Pros:</b> Clear division of roles and responsibilities among agents at different levels. Streamlined communication. Suitable for large systems with a structured decision flow.</p><p><b>Cons:</b> Failure at upper levels can disrupt the entire system. Lower-level agents have limited independence.</p><h3 id="6059">2.1.11 Custom multi-agent workflow</h3><p>Each agent communicates with only a subset of agents. Parts of the flow are deterministic, and only some agents can decide which other agents to call next.</p><p>In this architecture, we add individual agents as graph nodes and define the order in which agents are called ahead of time, in a custom workflow. In LangGraph the workflow can be defined in two ways:</p><ul><li><b>Explicit control flow (normal edges)</b>: LangGraph allows you to explicitly define the control flow of your application (i.e. the sequence of how agents communicate) explicitly, via <a href="https://langchain-ai.github.io/langgraph/concepts/low_level/#normal-edges" target="_blank">normal graph edges</a>. This is the most deterministic variant of this architecture above - we always know which agent will be called next ahead of time.</li><li><b>Dynamic control flow (<a href="https://langchain-ai.github.io/langgraph/concepts/low_level/#command" target="_blank">Command</a>)</b>: in LangGraph you can allow LLMs to decide parts of your application control flow. This can be achieved by using <code>Command</code>. A special case of this is a <a href="https://langchain-ai.github.io/langgraph/concepts/multi_agent/#supervisor-tool-calling" target="_blank">supervisor tool-calling</a> architecture. In that case, the tool-calling LLM powering the supervisor agent will make decisions about the order in which the tools (agents) are being called.</li></ul><pre><code class="language-python">from langchain_openai import ChatOpenAI\nfrom langgraph.graph import StateGraph, MessagesState, START\n\nmodel = ChatOpenAI()\n\ndef agent_1(state: MessagesState):\n    response = model.invoke(...)\n    return {"messages": [response]}\n\ndef agent_2(state: MessagesState):\n    response = model.invoke(...)\n    return {"messages": [response]}\n\nbuilder = StateGraph(MessagesState)\nbuilder.add_node(agent_1)\nbuilder.add_node(agent_2)\n# define the flow explicitly\nbuilder.add_edge(START, "agent_1")\nbuilder.add_edge("agent_1", "agent_2")</code></pre><h2 id="9c0b">3. Communication between agents</h2><p>The most important thing when building multi-agent systems is figuring out how the agents communicate. There are few different considerations:</p><ul><li>Do agents communicate via <b><a href="https://langchain-ai.github.io/langgraph/concepts/multi_agent/#graph-state-vs-tool-calls" target="_blank">via graph state or via tool calls</a></b>?</li><li>What if two agents have <b><a href="https://langchain-ai.github.io/langgraph/concepts/multi_agent/#different-state-schemas" target="_blank">different state schemas</a></b>?</li><li>How to communicate over a <b><a href="https://langchain-ai.github.io/langgraph/concepts/multi_agent/#shared-message-list" target="_blank">shared message list</a></b>?</li></ul><h3 id="71cf">3.1 Graph state vs tool calls</h3><p>What is the "payload" that is being passed around between agents? In most of the architectures discussed above the agents communicate via the <a href="https://langchain-ai.github.io/langgraph/concepts/low_level/#state" target="_blank">graph state</a>. In the case of the <a href="https://langchain-ai.github.io/langgraph/concepts/multi_agent/#supervisor-tool-calling" target="_blank">supervisor with tool-calling</a>, the payloads are tool call arguments.</p><img alt="Credits: Langchain" src="https://miro.medium.com/0*lMQ_Iib0qyXaXN9w.png"></img><p><b>Graph state</b></p><p>To communicate via graph state, individual agents need to be defined as <a href="https://langchain-ai.github.io/langgraph/concepts/low_level/#nodes" target="_blank">graph nodes</a>. These can be added as functions or as entire <a href="https://langchain-ai.github.io/langgraph/concepts/low_level/#subgraphs" target="_blank">subgraphs</a>. At each step of the graph execution, the agent node receives the current state of the graph, executes the agent code and then passes the updated state to the next nodes.</p><p>For example, in the code for Social Media Aggregator, we defined the Graph states as:</p><img alt="" src="https://miro.medium.com/1*mh1TiMe_bmy_cxXbP8EFXg.png"></img><p>Typically agent nodes share a single <a href="https://langchain-ai.github.io/langgraph/concepts/low_level/#schema" target="_blank">state schema</a>. However, you might want to design agent nodes with <a href="https://langchain-ai.github.io/langgraph/concepts/multi_agent/#different-state-schemas" target="_blank">different state schemas</a>.</p><h3 id="0437">3.2 Different state schemas</h3><p>An agent might need to have a different state schema from the rest of the agents. For example, a search agent might only need to keep track of queries and retrieved documents. There are two ways to achieve this in LangGraph:</p><ul><li>Define <a href="https://langchain-ai.github.io/langgraph/concepts/low_level/#subgraphs" target="_blank">subgraph</a> agents with a separate state schema. If there are no shared state keys (channels) between the subgraph and the parent graph, it\'s important to <a href="https://langchain-ai.github.io/langgraph/how-tos/subgraph-transform-state/" target="_blank">add input / output transformations</a> so that the parent graph knows how to communicate with the subgraphs.</li><li>Define agent node functions with a <a href="https://langchain-ai.github.io/langgraph/how-tos/pass_private_state/" target="_blank">private input state schema</a> that is distinct from the overall graph state schema. This allows passing information that is only needed for executing that particular agent.</li></ul><h3 id="10a1">3.3 Shared message list</h3><p>The most common way for the agents to communicate is via a shared state channel, typically a list of messages. This assumes that there is always at least a single channel (key) in the state that is shared by the agents. When communicating via a shared message list there is an additional consideration: should the agents <a href="https://langchain-ai.github.io/langgraph/concepts/multi_agent/#share-full-history" target="_blank">share the full history</a> of their thought process or only <a href="https://langchain-ai.github.io/langgraph/concepts/multi_agent/#share-final-result" target="_blank">the final result</a>?</p><img alt="Credits: Langchain" src="https://miro.medium.com/0*ae77qy7L9EcHfOY-.png"></img><p><b>Share full history</b></p><p>Agents can <b>share the full history</b> of their thought process (i.e. "scratchpad") with all other agents. This "scratchpad" would typically look like a <a href="https://langchain-ai.github.io/langgraph/concepts/low_level/#why-use-messages" target="_blank">list of messages</a>. The benefit of sharing full thought process is that it might help other agents make better decisions and improve reasoning ability for the system as a whole. The downside is that as the number of agents and their complexity grows, the "scratchpad" will grow quickly and might require additional strategies for <a href="https://langchain-ai.github.io/langgraph/concepts/memory/#managing-long-conversation-history" target="_blank">memory management</a>.</p><p><b>Share final result</b></p><p>Agents can have their own private "scratchpad" and only <b>share the final result</b> with the rest of the agents. This approach might work better for systems with many agents or agents that are more complex. In this case, you would need to define agents with <a href="https://langchain-ai.github.io/langgraph/concepts/multi_agent/#different-state-schemas" target="_blank">different state schemas</a></p><p>For agents called as tools, the supervisor determines the inputs based on the tool schema. Additionally, LangGraph allows <a href="https://langchain-ai.github.io/langgraph/how-tos/pass-run-time-values-to-tools/#pass-graph-state-to-tools" target="_blank">passing state</a> to individual tools at runtime, so subordinate agents can access parent state, if needed.</p><h2 id="2332">4. Conclusion</h2><p>Multi-agent LLM systems offer a powerful paradigm for tackling complex tasks by leveraging diverse architectural patterns like parallel, sequential, router, and aggregator workflows, as explored in this blog.</p><img alt="Image by Author" src="https://miro.medium.com/1*pSq718ln94kVTrrFC_CAqA.png"></img><p>Through a detailed examination of communication mechanisms such as shared state, message lists, and tool calls, we\'ve seen how agents collaborate to achieve seamless coordination.</p><h2 id="a5c8">Credits</h2><p>In this blog post, we have compiled information from various sources, including research papers, technical blogs, official documentations, YouTube videos, and more. Each source has been appropriately credited beneath the corresponding images, with source links provided.</p><h2 id="11e6">Thank you for reading!</h2><p>If this guide has enhanced your understanding of Python and Machine Learning:</p><ul><li>Please show your support with a clap 👏  or several claps!</li><li>Your claps help me create more valuable content for our vibrant Python or ML community.</li><li>Feel free to share this guide with fellow Python or AI / ML enthusiasts.</li><li>Your feedback is invaluable - it inspires and guides our future posts.</li></ul><h2 id="70bf">Connect with me!</h2><p><b><a href="https://www.linkedin.com/in/viprasingh/" target="_blank">Vipra</a></b></p></div>'}]
[2025-04-15 23:00:35] | DEBUG | [app:main:12] | [############ AI AGENT CONFIGURATIONS ############]
[2025-04-15 23:00:35] | DEBUG | [app:main:13] | [{'project': {'name': 'AI AGENT', 'description': 'Ai Agent Repository', 'company': 'Oguzhan', 'author': 'Mert Oğuzhan'}, 'logger': {'filepath': './logs/ai-agent.log', 'rotation': '50MB'}, 'agent': {'language_model_name': 'llama-3.3-70b-versatile', 'model_provider': 'groq'}, 'fastapi': {'host': '0.0.0.0', 'port': 9090, 'reload': True}}]
[2025-04-15 23:00:35] | INFO | [agent:__post_init__:20] | [post init worked successsfully!]
[2025-04-15 23:00:35] | INFO | [fastapi:run:41] | [Server Initialized!]
